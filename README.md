# ğŸ§  Vision4Healthcare

Welcome to **Vision4Healthcare**, my **virtual AI lab** ğŸ§ª â€” a place where I **experiment with cutting-edge deep learning algorithms**, share my **learning process**, and showcase **real-world medical imaging projects**.  
This repository reflects my continuous journey of **exploration, research, and implementation** in healthcare-focused computer vision.

> ğŸ”¬ Empowering health diagnostics with deep learning, innovation, and transparency.

---

## ğŸ“š Repository Structure

This repository contains organized modules, each focusing on a specific area of **medical imaging research**:

---

### ğŸ” 1. CNN-Based Medical Image Classification
Comparative analysis of multiple CNN architectures (e.g., ResNet, EfficientNet, DenseNet, Inception, MobileNet) applied to publicly available and balanced medical datasets.

#### ğŸ“‚ Projects:
- ğŸ–¼ï¸ [DeepCOVID-X: Comparative Analysis of CNN Architectures on COVID-19 Radiography](https://github.com/HussamUmer/Vision4Healthcare/tree/main/DeepCovid_X)

Each project includes:
- ğŸ“Š Model comparisons  
- ğŸ“ˆ Accuracy, precision, recall, F1-score  
- ğŸ§ª Confusion matrix & Grad-CAMs  
- âš–ï¸ Balanced dataset splits  
- ğŸ“œ Training logs & analysis notebooks

---

### ğŸ©» 2. Medical Image Segmentation Projects
This module focuses on **image segmentation tasks** in medical imaging using **U-Net** and other advanced architectures.  
Here, I explore algorithms that **identify, separate, and analyze specific regions** from medical scans such as **X-rays, MRIs, and CT images**.

#### ğŸ“‚ Projects:
- ğŸ§© [X-Ray Lung Segmentation using U-Net](https://github.com/HussamUmer/Vision4Healthcare/tree/main/XRay_UNet_Segmentation)
- ğŸ‘ï¸ [Retina Blood Vessel Segmentation using Attention U-Net](https://github.com/HussamUmer/Vision4Healthcare/tree/main/RetinaBloodVessel_AttentionUNet_Seg)
- ğŸ“¡ [UltraSeg-Bench: AbdomenUSMS Comparative Study](https://github.com/HussamUmer/Vision4Healthcare/tree/main/UltraSeg-Bench:%20AbdomenUSMS%20Comparative%20Study)
- ğŸ©º [Polyp Segmentation with U-Net and TransUNet on Kvasir-SEG](https://github.com/HussamUmer/Vision4Healthcare/tree/main/PolyP_Segmentation_Model_Comaparison)
- ğŸ”— **SegCompare â€” ISIC2016: UNet Family vs DeepLabV3+**  
  [https://github.com/HussamUmer/Vision4Healthcare/tree/main/SegCompare_ISIC2016_UNetFamily_DeepLabV3Plus](https://github.com/HussamUmer/Vision4Healthcare/tree/main/SegCompare_ISIC2016_UNetFamily_DeepLabV3Plus)

- ğŸš€ [Lite Yet Sharp: Gated Skips and Depthwise Decoders for Fast TransUNet Segmentation](https://github.com/HussamUmer/transunet-lite)

Each segmentation project includes:
- ğŸ§  Deep learning models for pixel-wise segmentation  
- ğŸ¨ Visualization of masks, overlays, and predictions  
- ğŸ“Š Training & validation metrics with Dice, IoU, Accuracy  
- ğŸ“œ Well-documented notebooks for reproducibility

---

### ğŸ”­ 3. Vision Transformer Projects
This module covers **Vision Transformers (ViT family)** for computer vision, focusing on **classification**, **detection**, **segmentation**, and **representation learning**. Projects typically compare backbones (e.g., **ViT/DeiT**, **Swin**, **ConvNeXt-ViT hybrids**, **Hybrid CNN+ViT**) across datasets and tasks, with strong emphasis on **robustness**, **efficiency**, and **interpretability**.

#### ğŸ“‚ Projects:
- ğŸ§¬ [MagFusion-ViT: Multi-Magnification Fusion with Vision Transformers for Robust Breast Histopathology Classification](https://github.com/HussamUmer/Vision4Healthcare/tree/main/MagFusion_ViT)
- ğŸ§  [AttentionViT-BCDiagnosis: Vision Transformer for Multi-Class Breast Cancer Histopathology](https://github.com/HussamUmer/AttentionViT-BCDiagnosis)

#### âœ… Each ViT project generally includes
- ğŸ§  **Backbones & Heads:** ViT/DeiT/Swin variants (ImageNet-pretrained) with task-specific heads (linear/classifier, DETR-style decoder, segmentation decoder).
- ğŸ“¦ **Datasets & Protocols:** Clear dataset splits (train/val/test), data cards, class balance notes, and optional domain-shift settings.
- âš™ï¸ **Training Setup:** AdamW, cosine LR + warmup, AMP mixed precision, gradient clipping, early stopping/checkpointing.
- ğŸ“Š **Metrics:**  
  - *Classification*: Top-1/Top-5 Acc, **Macro-F1**, ROC-AUC (if multi-label)  
  - *Detection*: mAP@[.50:.95]  
  - *Segmentation*: mIoU, Dice  
  - *Calibration*: ECE/reliability (optional)
- ğŸ”€ **Robustness & Generalization:** Cross-domain or cross-resolution tests; corruption/stain/augmentation stress tests; few-shot/low-data settings.
- â±ï¸ **Efficiency Reporting:** Latency (ms/img), throughput (img/s), peak GPU memory, parameter/FLOP counts; â€œefficiency frontierâ€ plots.
- ğŸ” **Interpretability:** Attention rollout/Grad-CAM, token/patch attribution, saliency maps, exemplar TP/FP/FN grids.
- ğŸ§ª **Ablations (Optional):** Augmentation strength, patch size, window size (Swin), drop-path/dropout, head depth, fine-tune vs linear probe.
- â™»ï¸ **Reproducibility:** Fixed seeds, frozen splits (path lists), `config.yaml` (env + hyperparams), saved checkpoints, run logs.
- ğŸš€ **Deployment (optional):** ONNX/ TorchScript export, INT8/FP16 benchmarking, batch-size/throughput trade-offs.

---

### ğŸ”„ 4. Knowledge Distillation
Implementation and evaluation of lightweight student models distilled from powerful teacher models for medical image analysis, enabling real-time performance on edge devices.

#### ğŸ“‚ Projects:
- ğŸ–¼ï¸ [MLFFAKD: Multilayer Feature Fusion Attention-Based Knowledge Distillation for White Blood Cell Detection](https://github.com/HussamUmer/MLFFAKD-White-Blood-Cell-Detection)

Each project includes:
- ğŸ“Š Teacher vs Student model performance comparisons  
- ğŸ“ˆ Accuracy, precision, recall, F1-score  
- ğŸ§ª Feature map visualizations & attention analysis  
- âš–ï¸ Balanced training and test dataset splits  
- ğŸ“œ Training logs, analysis notebooks & reproducible experiments

---

### ğŸ­ 5. GANs-Based Medical Image Translation
Leveraging Generative Adversarial Networks (GANs) for unpaired image-to-image translation tasks in medical imaging, such as stain normalization, domain adaptation, and synthetic data generation.

#### ğŸ“‚ Projects:
- ğŸ§¬ [TL-S-CycleGAN: Tumor Localization and Segmentation with CycleGAN Variants](https://github.com/HussamUmer/TL-S-CycleGAN-Histopathology)

Each project includes:
- ğŸ–¼ï¸ Unpaired image translation between benign and malignant, CT - MRI and vice versa, images
- ğŸ“Š Evaluation metrics: SSIM, PSNR, FID, and Dice coefficient
- ğŸ¨ Sample generated images showcasing domain adaptation
- ğŸ“œ Training and testing notebooks with visualizations
- ğŸ§ª Comparative analysis of different GANs and its variants

---

## ğŸ›£ï¸ Roadmap (Modules Coming Soon)

| Module | Description | ETA |
|--------|-------------|-----|
| ğŸ¯ Object Detection | YOLOv8, Faster R-CNN for lesion localization | Novâ€“Dec 2025 |
| ğŸ¤ Model Fusion & Ensembling | Combining CNN + ViT + GANs | Winter 2025/26 |
---

## ğŸ§¾ License

This project is licensed under the **MIT License**.  
Feel free to use, modify, and share with attribution.  
ğŸ“„ See [`LICENSE`](./LICENSE) for more details.

---

## ğŸ’¡ About

Created and maintained by **[Hussam Umer](https://github.com/HussamUmer)** â€“ a passionate AI researcher focused on making healthcare **more intelligent, accurate, and accessible** through deep learning.  

Follow the journey and stay tuned for more releases. âœ¨  
For feedback or collaboration: ğŸ“§ [Email Me](mailto:hussamumer28092000@gmail.com)

---

## ğŸŒ Badges

![Python](https://img.shields.io/badge/Python-3.10-blue.svg)
![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)
![Status](https://img.shields.io/badge/Status-Active-brightgreen)
