# ðŸ§ªðŸ§¬ **MagFusion-ViT: Multi-Magnification Fusion with Vision Transformers for Robust Breast Histopathology Classification**


[![Python](https://img.shields.io/badge/Python-3.10%2B-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.x-EE4C2C.svg)](https://pytorch.org/)
[![timm](https://img.shields.io/badge/timm-vision--transformers-50C878.svg)](https://github.com/huggingface/pytorch-image-models)
[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](#open-in-colab)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](#license)
[![Dataset: BreakHis](https://img.shields.io/badge/Dataset-BreakHis-8A2BE2.svg)](https://web.inf.ufpr.br/vri/databases/breast-cancer-histopathological-database-breakhis/)

---

## â“ **Why this study?**

Histopathology slides are captured at multiple magnifications (e.g., **100Ã—** and **400Ã—**). We compare **DeiT-Small** and **Swin-Tiny** on **single-magnification** vs **mixed-magnification** training to understand:

- ðŸ” whether mixing magnifications improves **generalization** and **robustness**  
- ðŸ§± which transformer backbone is more **scale-tolerant** to nuclei/texture patterns  
- ðŸ”„ how performance shifts **cross-magnification**

We report **macro-F1**, **balanced accuracy**, **per-class F1**, **confusion matrices**, and **latency/throughput**.

---

## ðŸ—ƒï¸ **Dataset**

- **Name:** BreakHis â€” *Breast Cancer Histopathological Database*  
- **Magnifications used:** **100Ã—**, **400Ã—**, and **Mixed (100Ã—+400Ã—)**  
- **Official page & access:** https://web.inf.ufpr.br/vri/databases/breast-cancer-histopathological-database-breakhis/  
- **Classes (8):** adenosis, ductal carcinoma, fibroadenoma, lobular carcinoma, mucinous carcinoma, papillary carcinoma, phyllodes tumor, tubular adenoma

### ðŸ“¦ **Our splits (per setup)**
- **Train:** 140 images/class  
- **Val:** 30 images/class  
- **Test:** 30 images/class

### ðŸ”— **Prepared splits (Google Drive)**
> These folders contain our **prepared per-setup splits** with the directory layout  
> `training/`, `validation/`, `testing/` (8 classes each), matching the counts above.

- **100Ã— Split:** https://drive.google.com/drive/folders/1m_4qZeVgjgaNhufJP6tRgP5z6Y6GakoC?usp=sharing  
- **400Ã— Split:** https://drive.google.com/drive/folders/1s9xMnK96_QO084fTWo8vqW5jWFv3WNXr?usp=sharing  
- **Mixed (100Ã— + 400Ã—) Split:** https://drive.google.com/drive/folders/1Rv-1K6j8HFEy6kaf3UJXIP4lPF3TE8cg?usp=sharing

---

## ðŸ§  **Methodology (Overview)**

### ðŸ—ï¸ **Backbones**
- **DeiT-Small** (ImageNet-1k pretrained)  
- **Swin-Tiny**  (ImageNet-1k pretrained)

### ðŸ”§ **Training setups (3)**
1. **100Ã— only**
2. **400Ã— only**
3. **Mixed (100Ã—+400Ã—)**

### âš™ï¸ **Training protocol (kept identical across models)**
- **Transforms:** RGB â†’ resize **224Ã—224** â†’ normalize (ImageNet mean/std)
- **Optimizer:** AdamW (lr=3e-4, weight_decay=0.05)
- **Schedule:** cosine decay with **5â€“10** warmup epochs
- **Runtime:** AMP mixed-precision, grad-clip=1.0, early stopping on **val macro-F1**, max 100 epochs
- **Logging:** train/val loss & acc, **macro-F1**, per-class F1, epoch/total time, peak GPU MB

### ðŸ“ **Evaluation**
- **In-domain:** Train=Test magnification (primary comparison)  
- **Cross-domain robustness:** Evaluate on other magnifications  
- **Metrics:** Accuracy, Balanced Accuracy, **Macro-F1**, Per-class PRF, Confusion Matrix, **Latency/Throughput**, Peak GPU MB  
- **Qualitative:** dataset previews, **t-SNE** (pretrained features), TP/FP/FN grids, attention rollout (DeiT)

---

## ðŸš€ **Open in Colab**

### ðŸ¤– **DeiT-Small runs**
| Run | Notebook | Notes |
|---|---|---|
| **DeiT-100Ã—** | [Open in Colab](https://colab.research.google.com/github/HussamUmer/Vision4Healthcare/blob/main/MagFusion_ViT/Notebooks/DEIT_100x%20%281%29.ipynb) | Train on **100Ã—**; primary test **100Ã—**; robustness: **400Ã—**, **Mixed** |
| **DeiT-400Ã—** | [Open in Colab](https://colab.research.google.com/github/HussamUmer/Vision4Healthcare/blob/main/MagFusion_ViT/Notebooks/DEIT_400x.ipynb) | Train on **400Ã—**; primary test **400Ã—**; robustness: **100Ã—**, **Mixed** |
| **DeiT-Mixed** | [Open in Colab](https://colab.research.google.com/github/HussamUmer/Vision4Healthcare/blob/main/MagFusion_ViT/Notebooks/DEIT_Mixed.ipynb) | Train on **Mixed**; primary test **Mixed**; robustness: **100Ã—**, **400Ã—** |

### ðŸªŸ **Swin-Tiny runs**
| Run | Notebook | Notes |
|---|---|---|
| **Swin-Tiny-100Ã—** | [Open in Colab](https://colab.research.google.com/github/HussamUmer/Vision4Healthcare/blob/main/MagFusion_ViT/Notebooks/SwinTiny_100x%20%281%29.ipynb) | Train on **100Ã—**; primary test **100Ã—**; robustness: **400Ã—**, **Mixed** |
| **Swin-Tiny-400Ã—** | [Open in Colab](https://colab.research.google.com/github/HussamUmer/Vision4Healthcare/blob/main/MagFusion_ViT/Notebooks/SwinTiny_400x.ipynb) | Train on **400Ã—**; primary test **400Ã—**; robustness: **100Ã—**, **Mixed** |
| **Swin-Tiny-Mixed** | [Open in Colab](https://colab.research.google.com/github/HussamUmer/Vision4Healthcare/blob/main/MagFusion_ViT/Notebooks/SwinTiny_Mixed.ipynb) | Train on **Mixed**; primary test **Mixed**; robustness: **100Ã—**, **400Ã—** |

---

## âš¡ **Quick Start (Colab)**

1. ðŸ”— Open a notebook above and **Run All**  
2. ðŸ’¾ Mount Google Drive; set dataset roots for **100Ã—**, **400Ã—**, and **Mixed**  
3. ðŸƒ Run **Data checks â†’ Training â†’ Final Testing â†’ Reports**  
4. ðŸ“¦ Artifacts (checkpoints, logs, plots) are saved in a timestamped **`RUN_DIR`** under your Drive

---

## ðŸ” **Reproducibility**

- ðŸŽ¯ Fixed seeds (Python/NumPy/PyTorch) + deterministic cuDNN flags  
- ðŸ§¾ Frozen splits saved as path lists; `config.yaml` logs software + hardware  
- ðŸ… Best checkpoint = **highest validation macro-F1** (tie-break: val accuracy)

---

## ðŸ“Š **Results Summary â€” MagFusion-ViT (DeiT-Small & Swin-Tiny across 100Ã— / 400Ã— / Mixed)**

Quick takeaways:
- âœ… **In-domain** (train=test) is near-perfect on **400Ã—** for both models (Macro-F1 â‰ˆ **0.992â€“0.996**).
- ðŸ§ª **Cross-domain** drops are **asymmetric**: training on **400Ã— â†’ testing on 100Ã—** drops more than the reverse.
- ðŸ§© **Mixed training** improves **robustness**. **Swin-Tiny (Mixed)** generalizes best overall (Macro-F1 â‰ˆ **0.955â€“0.963** on 100Ã—/400Ã—/Mixed).

---

## ðŸ§· Primary (In-Domain) Performance â€” Headline Metrics
*Each row reports the primary evaluation where **Train = Test** magnification. Includes efficiency stats to aid practical comparisons.*

| Model | Train=Test Setup | Macro-F1 (â†‘) | Accuracy (â†‘) | Balanced Acc (â†‘) | Latency (ms/img) (â†“) | Throughput (img/s) (â†‘) | Peak GPU (MB) |
|---|---|---:|---:|---:|---:|---:|---:|
| **DeiT-Small** | 100Ã— | **0.979** | 0.979 | 0.979 | 166.581 | 6.003 | 528.254 |
| **DeiT-Small** | 400Ã— | **0.996** | 0.996 | 0.996 | 704.081 | 1.420 | 528.254 |
| **DeiT-Small** | Mixed | **0.920** | 0.921 | 0.921 | 477.284 | 2.095 | 528.004 |
| **Swin-Tiny** | 100Ã— | **0.958** | 0.958 | 0.958 | 303.168 | 3.298 | 1302.659 |
| **Swin-Tiny** | 400Ã— | **0.992** | 0.992 | 0.992 | 269.120 | 3.716 | 1302.659 |
| **Swin-Tiny** | Mixed | **0.963** | 0.963 | 0.963 | 224.129 | 4.462 | 1302.659 |

**What it shows:** how well each model performs on the distribution it was trained on, plus runtime/memory.  
**Observation:** Both models excel on **400Ã—** in-domain; **Swin-Tiny (Mixed)** reaches strong, balanced in-domain performance with the **best latency** among Swin runs.

---

## ðŸ”€ Cross-Domain Robustness â€” Macro-F1 (â†‘)

**What matters here:** performance when **train and test magnifications differ**.  
**Key findings (at a glance):**
- **Mixed training** gives the most reliable cross-domain behavior; **Swin-Tiny (Mixed)** is the most consistent (Macro-F1 â‰ˆ **0.955â€“0.963** to both 100Ã— and 400Ã—).  
- **Directional gap is asymmetric:** going **400Ã— â†’ 100Ã—** is harder than **100Ã— â†’ 400Ã—** for both models.  
- Averaging all off-diagonal entries, **DeiT â‰ˆ 0.713** and **Swin â‰ˆ 0.644**; using a **Mixed recipe** is the simplest way to close the gap.

---

### ðŸ§­ Cross-Domain Macro-F1 Matrices (for reference; diagonals are in-domain)

#### DeiT-Small â€” Train Ã— Test (Macro-F1 â†‘)
| Train \ Test | 100Ã— | 400Ã— | Mixed |
|---|---:|---:|---:|
| **100Ã—** | 0.979 *(in-domain)* | **0.518** | **0.769** |
| **400Ã—** | **0.405** | 0.996 *(in-domain)* | **0.734** |
| **Mixed** | **0.921** | **0.933** | 0.920 *(in-domain)* |

#### Swin-Tiny â€” Train Ã— Test (Macro-F1 â†‘)
| Train \ Test | 100Ã— | 400Ã— | Mixed |
|---|---:|---:|---:|
| **100Ã—** | 0.958 *(in-domain)* | **0.337** | **0.681** |
| **400Ã—** | **0.245** | 0.992 *(in-domain)* | **0.685** |
| **Mixed** | **0.955** | **0.963** | 0.963 *(in-domain)* |

> **Read:** focus on the **bold off-diagonal** cellsâ€”those are the cross-domain results.

![In-Domain vs Cross-Domain (use the orange bars)](https://raw.githubusercontent.com/HussamUmer/Vision4Healthcare/main/MagFusion_ViT/Outputs/Graphs/Cross_Domain_Robustness/indomain_vs_crossdomain.png)

<sub><b>Figure 1.</b> Averages of **in-domain** (blue, diagonal) vs **cross-domain** (orange, off-diagonal). For this section, focus on **cross-domain (orange)** to gauge robustness.</sub>

---

### ðŸ“ Directional Asymmetry (train â†’ test)
| Model | 100Ã— â†’ 400Ã— | 400Ã— â†’ 100Ã— |
|---|---:|---:|
| **DeiT-Small** | **0.518** | **0.405** |
| **Swin-Tiny**  | **0.337** | **0.245** |

> **Interpretation:** **400Ã— â†’ 100Ã—** consistently underperforms **100Ã— â†’ 400Ã—**, suggesting models trained on high-mag textures struggle to generalize down to lower magnification.

![Directional Generalization Asymmetry](https://raw.githubusercontent.com/HussamUmer/Vision4Healthcare/main/MagFusion_ViT/Outputs/Graphs/Cross_Domain_Robustness/generalization.png)

<sub><b>Figure 2.</b> **Directional gap** between 100Ã—â†’400Ã— and 400Ã—â†’100Ã—. Both models struggle more when moving **down** in magnification (400Ã—â†’100Ã—).</sub>

---

### ðŸ§ª Generalization from **Mixed** Training (cross-domain only)
| Model | Mixed â†’ 100Ã— | Mixed â†’ 400Ã— |
|---|---:|---:|
| **DeiT-Small** | **0.921** | **0.933** |
| **Swin-Tiny**  | **0.955** | **0.963** |

> **Interpretation:** Mixed training substantially reduces domain shiftâ€”**Swin-Tiny (Mixed)** is the most robust, with near-symmetric performance to both 100Ã— and 400Ã—.

![Generalization from Mixed Training](https://raw.githubusercontent.com/HussamUmer/Vision4Healthcare/main/MagFusion_ViT/Outputs/Graphs/Cross_Domain_Robustness/generalization_from_mixed.png)
 
<sub><b>Figure 3.</b> Cross-domain generalization from **Mixed training**. Bars show Macro-F1 on 100Ã— and 400Ã— tests (ignore Mixedâ†’Mixed as itâ€™s in-domain). **Swin-Tiny (Mixed)** is strongest and most symmetric.</sub>

---

### ðŸ§® Cross-Domain Mean (average of all off-diagonal cells)
| Model | Mean Macro-F1 (â†‘) |
|---|---:|
| **DeiT-Small** | **0.713** |
| **Swin-Tiny**  | **0.644** |

> **Interpretation:** On average across all cross-domain conditions, **DeiT** edges **Swin**â€”but **Swin-Tiny (Mixed)** is the **best single recipe** if you can only train once and must handle both magnifications at test time.

![Cross-Domain Robustness Heatmaps](https://raw.githubusercontent.com/HussamUmer/Vision4Healthcare/main/MagFusion_ViT/Outputs/Graphs/Cross_Domain_Robustness/crossdomain_robustness.png)

<sub><b>Figure 4.</b> Heatmaps of TrainÃ—Test Macro-F1. Emphasize the **off-diagonal** cells. Mixed rows are uniformly high, especially for **Swin-Tiny**.</sub>

---

## â±ï¸ Efficiency Snapshot â€” In-Domain Only

**What this shows (quick read):**
- **DeiT-Small** has the **lowest VRAM footprint (~528 MB)** and is **fastest** when trained/tested on **100Ã—** (â‰ˆ**166.6 ms/img**, **6.00 img/s**).
- **Swin-Tiny** is **faster** on **Mixed** and **400Ã—** among its own runs (down to **224.1 ms/img**, up to **4.46 img/s**), but uses **more VRAM (~1.3 GB)**.
- If **memory is tight**, DeiT-Small is the practical choice; if you need **speed at higher mags/mixed**, Swin-Tiny wins within its family.

### ðŸ“‹ Table (Train = Test for each run)

| Model       | Train=Test | Latency (ms/img) â†“ | Throughput (img/s) â†‘ | Peak GPU (MB) |
|---|---|---:|---:|---:|
| **DeiT-Small** | 100Ã—  | **166.581** | **6.003** | **528.254** |
| **DeiT-Small** | 400Ã—  | 704.081 | 1.420 | 528.254 |
| **DeiT-Small** | Mixed | 477.284 | 2.095 | 528.004 |
| **Swin-Tiny**  | 100Ã—  | 303.168 | 3.298 | 1302.659 |
| **Swin-Tiny**  | 400Ã—  | 269.120 | 3.716 | 1302.659 |
| **Swin-Tiny**  | Mixed | **224.129** | **4.462** | 1302.659 |

> These are **in-domain** numbers (each model evaluated on the distribution it was trained on).

![Efficiency Frontier â€” In-Domain Only](https://raw.githubusercontent.com/HussamUmer/Vision4Healthcare/main/MagFusion_ViT/Outputs/Graphs/Efficiency_Snapshot/efficiency_frontier.png)
<sub><b>Figure 5.</b> In-domain efficiency frontier â€” <b>latency vs throughput</b>; bubble size = peak GPU MB; color = setup; marker = model.</sub>

---

### â±ï¸ Latency (ms/image, â†“) â€” In-Domain Only
| Model | 100Ã— | 400Ã— | Mixed |
|---|---:|---:|---:|
| **DeiT-Small** | **166.581** | 704.081 | 477.284 |
| **Swin-Tiny**  | 303.168 | 269.120 | **224.129** |

> **Read:** lower is better. **DeiT-Small** excels on **100Ã—**; **Swin-Tiny** excels on **Mixed**/**400Ã—**.

![Latency (ms/img) â€” In-Domain Only](https://raw.githubusercontent.com/HussamUmer/Vision4Healthcare/main/MagFusion_ViT/Outputs/Graphs//Efficiency_Snapshot/latency.png)  
<sub><b>Figure 6.</b> In-domain efficiency view â€” <b>latency</b> (ms/img). Lower is better.</sub>

---

### ðŸš€ Throughput (images/sec, â†‘) â€” In-Domain Only
| Model | 100Ã— | 400Ã— | Mixed |
|---|---:|---:|---:|
| **DeiT-Small** | **6.003** | 1.420 | 2.095 |
| **Swin-Tiny**  | 3.298 | 3.716 | **4.462** |

> **Read:** higher is better. **DeiT-Small** is the fastest on **100Ã—** overall; **Swin-Tiny** is fastest for **Mixed**.

![Throughput (img/s) â€” In-Domain Only](https://raw.githubusercontent.com/HussamUmer/Vision4Healthcare/main/MagFusion_ViT/Outputs/Graphs//Efficiency_Snapshot/throughput.png)  
<sub><b>Figure 7.</b> In-domain efficiency view â€” <b>throughput</b> (img/s). Higher is better.</sub>

---

### ðŸ§  Peak GPU Memory (MB, â†“) â€” In-Domain Only
| Model | 100Ã— | 400Ã— | Mixed |
|---|---:|---:|---:|
| **DeiT-Small** | **528.254** | **528.254** | **528.004** |
| **Swin-Tiny**  | 1302.659 | 1302.659 | 1302.659 |

> **Read:** **DeiT-Small** is â‰ˆ**2.5Ã—** more memory-efficient than **Swin-Tiny**.

![Peak GPU Memory (MB) â€” In-Domain Only](https://raw.githubusercontent.com/HussamUmer/Vision4Healthcare/main/MagFusion_ViT/Outputs/Graphs//Efficiency_Snapshot/Peak_GPU.png)    
<sub><b>Figure 8.</b> In-domain efficiency view â€” <b>peak GPU memory</b> (MB). Lower is better.</sub>

---

### ðŸ â€œBest per Familyâ€ (quick picks)
| Model | Fastest Setup | Latency â†“ | Throughput â†‘ | Peak GPU (MB) |
|---|---|---:|---:|---:|
| **DeiT-Small** | **100Ã—** | **166.581** | **6.003** | **528.254** |
| **Swin-Tiny**  | **Mixed** | **224.129** | **4.462** | 1302.659 |

> **Interpretation:** choose **DeiT-Small** when VRAM is tight or the target is **100Ã—**; choose **Swin-Tiny** when you expect **mixed/400Ã—** test conditions and have more memory.

---

## ðŸ§© Subgroup Summary (Macro-F1, Acc, BalAcc) â€” Per Test Distribution
*Handy for quick graphing across test sets; combine rows from the three runs of each model.*

**At a glance**
- For **100Ã— tests**, **DeiT-Small** is both **more accurate** and **faster** (Macro-F1â‰ˆ0.979; 166.6 ms/img).
- For **400Ã— tests**, **both models** peak (Macro-F1â‰ˆ0.992â€“0.996); **Swin-Tiny** is quicker.
- For **Mixed tests**, **Swin-Tiny** leads in both **accuracy** (â‰ˆ0.963) and **speed** (â‰ˆ224 ms/img, 4.46 img/s).

---

### Swin-Tiny â€” by Test Distribution
| Test Group | Macro-F1 | Acc | BalAcc | Latency (ms/img) | Throughput (img/s) |
|---|---:|---:|---:|---:|---:|
| **100Ã—** (from 100Ã— run) | 0.958 | 0.958 | 0.958 | 303.168 | 3.298 |
| **400Ã—** (from 400Ã— run) | 0.992 | 0.992 | 0.992 | 269.120 | 3.716 |
| **Mixed** (from Mixed run) | 0.963 | 0.963 | 0.963 | 224.129 | 4.462 |

### DeiT-Small â€” by Test Distribution
| Test Group | Macro-F1 | Acc | BalAcc | Latency (ms/img) | Throughput (img/s) |
|---|---:|---:|---:|---:|---:|
| **100Ã—** (from 100Ã— run) | 0.979 | 0.979 | 0.979 | 166.581 | 6.003 |
| **400Ã—** (from 400Ã— run) | 0.996 | 0.996 | 0.996 | 704.081 | 1.420 |
| **Mixed** (from Mixed run) | 0.920 | 0.921 | 0.921 | 477.284 | 2.095 |

**What it shows:** a compact â€œbest per test distributionâ€ view, useful for **bar charts** (Macro-F1, Acc, BalAcc) and **runtime plots**.

---

### 1) Latency (ms/img) â€” In-Domain per Test Distribution
| Test Distribution | DeiT-Small (â†“) | Swin-Tiny (â†“) | Winner |
|---|---:|---:|---|
| **100Ã—** | **166.581** | 303.168 | **DeiT-Small** |
| **400Ã—** | 704.081 | **269.120** | **Swin-Tiny** |
| **Mixed** | 477.284 | **224.129** | **Swin-Tiny** |

> **Interpretation:** DeiT-Small is fastest on **100Ã—**; Swin-Tiny is faster on **400Ã—** and **Mixed**.

![Latency (ms/img) â€” In-Domain per Test Distribution](https://raw.githubusercontent.com/HussamUmer/Vision4Healthcare/main/MagFusion_ViT/Outputs/Graphs//subgroup_summary/latency.png)    
<sub><b>Figure 9.</b> Latency by test distribution (lower is better).</sub>

---

### 2) Subgroup Summary â€” Macro-F1 / Acc / BalAcc (Per Test Distribution)

**Swin-Tiny**
| Test | Macro-F1 | Acc | BalAcc |
|---|---:|---:|---:|
| **100Ã—** | 0.958 | 0.958 | 0.958 |
| **400Ã—** | 0.992 | 0.992 | 0.992 |
| **Mixed** | 0.963 | 0.963 | 0.963 |

**DeiT-Small**
| Test | Macro-F1 | Acc | BalAcc |
|---|---:|---:|---:|
| **100Ã—** | 0.979 | 0.979 | 0.979 |
| **400Ã—** | 0.996 | 0.996 | 0.996 |
| **Mixed** | 0.920 | 0.921 | 0.921 |

> **Interpretation:** Both models peak at **400Ã—**; **DeiT-Small** leads on **100Ã—**, while **Swin-Tiny** leads on **Mixed**.

![Subgroup Summary â€” Macro-F1 / Acc / BalAcc (Per Test Distribution)](https://raw.githubusercontent.com/HussamUmer/Vision4Healthcare/main/MagFusion_ViT/Outputs/Graphs//subgroup_summary/summary.png)
<sub><b>Figure 10.</b> Macro-F1, Accuracy, and Balanced Accuracy per test distribution for each model.</sub>


---

### 3) Throughput (img/s) â€” In-Domain per Test Distribution
| Test Distribution | DeiT-Small (â†‘) | Swin-Tiny (â†‘) | Winner |
|---|---:|---:|---|
| **100Ã—** | **6.003** | 3.298 | **DeiT-Small** |
| **400Ã—** | 1.420 | **3.716** | **Swin-Tiny** |
| **Mixed** | 2.095 | **4.462** | **Swin-Tiny** |

> **Interpretation:** Higher is better. DeiT-Small is the fastest on **100Ã—** overall; Swin-Tiny is faster on **400Ã—** and **Mixed**.

![Throughput (img/s) â€” In-Domain per Test Distribution](https://raw.githubusercontent.com/HussamUmer/Vision4Healthcare/main/MagFusion_ViT/Outputs/Graphs//subgroup_summary/throughput.png)  
<sub><b>Figure 11.</b> Throughput by test distribution (higher is better).</sub>

---

### 4) Efficiency Frontier Points â€” In-Domain per Test Distribution  
*(Scatter shows each point; table lists exact coordinates & VRAM.)*
| Model      | Test | Latency (ms/img) â†“ | Throughput (img/s) â†‘ | Peak GPU (MB) |
|---|---|---:|---:|---:|
| **DeiT-Small** | 100Ã—  | **166.581** | **6.003** | **528.254** |
| **DeiT-Small** | 400Ã—  | 704.081 | 1.420 | 528.254 |
| **DeiT-Small** | Mixed | 477.284 | 2.095 | 528.004 |
| **Swin-Tiny**  | 100Ã—  | 303.168 | 3.298 | 1302.659 |
| **Swin-Tiny**  | 400Ã—  | 269.120 | 3.716 | 1302.659 |
| **Swin-Tiny**  | Mixed | 224.129 | 4.462 | 1302.659 |

> **Interpretation:** **DeiT-Small (100Ã—)** sits at the **upper-left** (lowest latency, highest throughput). **Swin-Tiny (Mixed)** is the fastest Swin point; **DeiT-Small (400Ã—)** is furthest from the frontier.

![Efficiency Frontier â€” In-Domain per Test Distribution](https://raw.githubusercontent.com/HussamUmer/Vision4Healthcare/main/MagFusion_ViT/Outputs/Graphs//subgroup_summary/efficiency.png)  
<sub><b>Figure 12.</b> Efficiency frontier (latency vs throughput); color = test set, marker = model.</sub>

---

## ðŸ§© In-Domain Confusion Matrices (DeiT-Small & Swin-Tiny)

**What this shows:** Per-setup, in-domain performance (Train = Test). Each matrix is **row-normalized** (values sum to 1 per true class) so off-diagonal intensity reflects misclassification patterns. All plots use the **same class order** and **same color scale** for fair visual comparison.

---

### ðŸ¤– DeiT-Small â€” In-Domain CMs (Train = Test)

| 100Ã— | 400Ã— | Mixed |
|---|---|---|
| ![DeiT-Small â€” 100Ã— (in-domain CM)](https://raw.githubusercontent.com/HussamUmer/Vision4Healthcare/main/MagFusion_ViT/Outputs/Graphs/confusion_matrices/deitsmall/indomain/100x.png) | ![DeiT-Small â€” 400Ã— (in-domain CM)](https://raw.githubusercontent.com/HussamUmer/Vision4Healthcare/main/MagFusion_ViT/Outputs/Graphs/confusion_matrices/deitsmall/indomain/400x.png) | ![DeiT-Small â€” Mixed (in-domain CM)](https://raw.githubusercontent.com/HussamUmer/Vision4Healthcare/main/MagFusion_ViT/Outputs/Graphs/confusion_matrices/deitsmall/indomain/mixed.png) |

---

### ðŸ¤– Swin-Tiny â€” In-Domain CMs (Train = Test)

| 100Ã— | 400Ã— | Mixed |
|---|---|---|
| ![Sin-Tiny â€” 100Ã— (in-domain CM)](https://raw.githubusercontent.com/HussamUmer/Vision4Healthcare/main/MagFusion_ViT/Outputs/Graphs/confusion_matrices/swintiny/indomain/100x.png) | ![Swin-Tiny â€” 400Ã— (in-domain CM)](https://raw.githubusercontent.com/HussamUmer/Vision4Healthcare/main/MagFusion_ViT/Outputs/Graphs/confusion_matrices/swintiny/indomain/400x.png) | ![Swin-Tiny â€” Mixed (in-domain CM)](https://raw.githubusercontent.com/HussamUmer/Vision4Healthcare/main/MagFusion_ViT/Outputs/Graphs/confusion_matrices/swintiny/indomain/mixed.png) |

---

## ðŸ“¦ Experiment Artifacts & Checkpoints (Google Drive)

Large files (logs, checkpoints, evaluation CSV/JSON, and figures) are hosted on Google Drive:

ðŸ”— **Drive folder:** https://drive.google.com/drive/folders/1qhvplLgcpmJn7D1f0HVC69GvwmzjEhwa?usp=sharing

**Contents (typical):**
- `runs/` â€” timestamped run directories (e.g., `2025-09-22_deit_100x/`)
  - `config.yaml` (env, hyperparams, data roots)
  - `train_log.csv` / `events.*` (training & validation curves)
  - `best.ckpt` (checkpoint selected by validation macro-F1)
  - `eval/` (JSON/CSV metrics, confusion matrices, latency/throughput)
  - `figures/` (plots used in the README/paper)
- `splits/` â€” frozen train/val/test path lists for reproducibility

---

## ðŸ“š **Citations (Background)**

- **DeiT:** Touvron et al., *Training data-efficient image transformers & distillation through attention*. ICML 2021.  
- **Swin Transformer:** Liu et al., *Swin Transformer: Hierarchical Vision Transformer using Shifted Windows*. 2021.  
- **BreakHis:** Spanhol et al., *A Dataset for Breast Cancer Histopathological Image Classification*. IEEE T-BioCAS 2016.

*(Please cite the original papers and the dataset per their licenses.)*

---

## ðŸ“œ **License**

Released under the **MIT License**. See [`LICENSE`](LICENSE) for details.
