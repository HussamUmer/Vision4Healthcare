# üß™üß¨ **MagFusion-ViT: Multi-Magnification Fusion with Vision Transformers for Robust Breast Histopathology Classification**


[![Python](https://img.shields.io/badge/Python-3.10%2B-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.x-EE4C2C.svg)](https://pytorch.org/)
[![timm](https://img.shields.io/badge/timm-vision--transformers-50C878.svg)](https://github.com/huggingface/pytorch-image-models)
[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](#open-in-colab)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](#license)
[![Dataset: BreakHis](https://img.shields.io/badge/Dataset-BreakHis-8A2BE2.svg)](https://web.inf.ufpr.br/vri/databases/breast-cancer-histopathological-database-breakhis/)

---

## ‚ùì **Why this study?**

Histopathology slides are captured at multiple magnifications (e.g., **100√ó** and **400√ó**). We compare **DeiT-Small** and **Swin-Tiny** on **single-magnification** vs **mixed-magnification** training to understand:

- üîÅ whether mixing magnifications improves **generalization** and **robustness**  
- üß± which transformer backbone is more **scale-tolerant** to nuclei/texture patterns  
- üîÑ how performance shifts **cross-magnification**

We report **macro-F1**, **balanced accuracy**, **per-class F1**, **confusion matrices**, and **latency/throughput**.

---

## üóÉÔ∏è **Dataset**

- **Name:** BreakHis ‚Äî *Breast Cancer Histopathological Database*  
- **Magnifications used:** **100√ó**, **400√ó**, and **Mixed (100√ó+400√ó)**  
- **Official page & access:** https://web.inf.ufpr.br/vri/databases/breast-cancer-histopathological-database-breakhis/  
- **Classes (8):** adenosis, ductal carcinoma, fibroadenoma, lobular carcinoma, mucinous carcinoma, papillary carcinoma, phyllodes tumor, tubular adenoma

### üì¶ **Our splits (per setup)**
- **Train:** 140 images/class  
- **Val:** 30 images/class  
- **Test:** 30 images/class

---

## üß† **Methodology (Overview)**

### üèóÔ∏è **Backbones**
- **DeiT-Small** (ImageNet-1k pretrained)  
- **Swin-Tiny**  (ImageNet-1k pretrained)

### üîß **Training setups (3)**
1. **100√ó only**
2. **400√ó only**
3. **Mixed (100√ó+400√ó)**

### ‚öôÔ∏è **Training protocol (kept identical across models)**
- **Transforms:** RGB ‚Üí resize **224√ó224** ‚Üí normalize (ImageNet mean/std)
- **Optimizer:** AdamW (lr=3e-4, weight_decay=0.05)
- **Schedule:** cosine decay with **5‚Äì10** warmup epochs
- **Runtime:** AMP mixed-precision, grad-clip=1.0, early stopping on **val macro-F1**, max 100 epochs
- **Logging:** train/val loss & acc, **macro-F1**, per-class F1, epoch/total time, peak GPU MB

### üìè **Evaluation**
- **In-domain:** Train=Test magnification (primary comparison)  
- **Cross-domain robustness:** Evaluate on other magnifications  
- **Metrics:** Accuracy, Balanced Accuracy, **Macro-F1**, Per-class PRF, Confusion Matrix, **Latency/Throughput**, Peak GPU MB  
- **Qualitative:** dataset previews, **t-SNE** (pretrained features), TP/FP/FN grids, attention rollout (DeiT)

---

## üöÄ **Open in Colab**

### ü§ñ **DeiT-Small runs**
| Run | Notebook | Notes |
|---|---|---|
| **DeiT-100√ó** | [Open in Colab](https://colab.research.google.com/github/<USER>/<REPO>/blob/main/notebooks/deit_100x.ipynb) | Train on **100√ó**; primary test **100√ó**; robustness: **400√ó**, **Mixed** |
| **DeiT-400√ó** | [Open in Colab](https://colab.research.google.com/github/<USER>/<REPO>/blob/main/notebooks/deit_400x.ipynb) | Train on **400√ó**; primary test **400√ó**; robustness: **100√ó**, **Mixed** |
| **DeiT-Mixed** | [Open in Colab](https://colab.research.google.com/github/<USER>/<REPO>/blob/main/notebooks/deit_mixed.ipynb) | Train on **Mixed**; primary test **Mixed**; robustness: **100√ó**, **400√ó** |

### ü™ü **Swin-Tiny runs**
| Run | Notebook | Notes |
|---|---|---|
| **Swin-Tiny-100√ó** | [Open in Colab](https://colab.research.google.com/github/<USER>/<REPO>/blob/main/notebooks/swin_tiny_100x.ipynb) | Train on **100√ó**; primary test **100√ó**; robustness: **400√ó**, **Mixed** |
| **Swin-Tiny-400√ó** | [Open in Colab](https://colab.research.google.com/github/<USER>/<REPO>/blob/main/notebooks/swin_tiny_400x.ipynb) | Train on **400√ó**; primary test **400√ó**; robustness: **100√ó**, **Mixed** |
| **Swin-Tiny-Mixed** | [Open in Colab](https://colab.research.google.com/github/<USER>/<REPO>/blob/main/notebooks/swin_tiny_mixed.ipynb) | Train on **Mixed**; primary test **Mixed**; robustness: **100√ó**, **400√ó** |

---

## ‚ö° **Quick Start (Colab)**

1. üîó Open a notebook above and **Run All**  
2. üíæ Mount Google Drive; set dataset roots for **100√ó**, **400√ó**, and **Mixed**  
3. üèÉ Run **Data checks ‚Üí Training ‚Üí Final Testing ‚Üí Reports**  
4. üì¶ Artifacts (checkpoints, logs, plots) are saved in a timestamped **`RUN_DIR`** under your Drive

---

## üîÅ **Reproducibility**

- üéØ Fixed seeds (Python/NumPy/PyTorch) + deterministic cuDNN flags  
- üßæ Frozen splits saved as path lists; `config.yaml` logs software + hardware  
- üèÖ Best checkpoint = **highest validation macro-F1** (tie-break: val accuracy)

---

## üìä **Results Summary ‚Äî MagFusion-ViT (DeiT-Small & Swin-Tiny across 100√ó / 400√ó / Mixed)**

Quick takeaways:
- ‚úÖ **In-domain** (train=test) is near-perfect on **400√ó** for both models (Macro-F1 ‚âà **0.992‚Äì0.996**).
- üß™ **Cross-domain** drops are **asymmetric**: training on **400√ó ‚Üí testing on 100√ó** drops more than the reverse.
- üß© **Mixed training** improves **robustness**. **Swin-Tiny (Mixed)** generalizes best overall (Macro-F1 ‚âà **0.955‚Äì0.963** on 100√ó/400√ó/Mixed).

---

## üß∑ Primary (In-Domain) Performance ‚Äî Headline Metrics
*Each row reports the primary evaluation where **Train = Test** magnification. Includes efficiency stats to aid practical comparisons.*

| Model | Train=Test Setup | Macro-F1 (‚Üë) | Accuracy (‚Üë) | Balanced Acc (‚Üë) | Latency (ms/img) (‚Üì) | Throughput (img/s) (‚Üë) | Peak GPU (MB) |
|---|---|---:|---:|---:|---:|---:|---:|
| **DeiT-Small** | 100√ó | **0.979** | 0.979 | 0.979 | 166.581 | 6.003 | 528.254 |
| **DeiT-Small** | 400√ó | **0.996** | 0.996 | 0.996 | 704.081 | 1.420 | 528.254 |
| **DeiT-Small** | Mixed | **0.920** | 0.921 | 0.921 | 477.284 | 2.095 | 528.004 |
| **Swin-Tiny** | 100√ó | **0.958** | 0.958 | 0.958 | 303.168 | 3.298 | 1302.659 |
| **Swin-Tiny** | 400√ó | **0.992** | 0.992 | 0.992 | 269.120 | 3.716 | 1302.659 |
| **Swin-Tiny** | Mixed | **0.963** | 0.963 | 0.963 | 224.129 | 4.462 | 1302.659 |

**What it shows:** how well each model performs on the distribution it was trained on, plus runtime/memory.  
**Observation:** Both models excel on **400√ó** in-domain; **Swin-Tiny (Mixed)** reaches strong, balanced in-domain performance with the **best latency** among Swin runs.

---

## üîÄ Cross-Domain Robustness ‚Äî Macro-F1 (‚Üë)

**What matters here:** performance when **train and test magnifications differ**.  
**Key findings (at a glance):**
- **Mixed training** gives the most reliable cross-domain behavior; **Swin-Tiny (Mixed)** is the most consistent (Macro-F1 ‚âà **0.955‚Äì0.963** to both 100√ó and 400√ó).  
- **Directional gap is asymmetric:** going **400√ó ‚Üí 100√ó** is harder than **100√ó ‚Üí 400√ó** for both models.  
- Averaging all off-diagonal entries, **DeiT ‚âà 0.713** and **Swin ‚âà 0.644**; using a **Mixed recipe** is the simplest way to close the gap.

---

### üß≠ Cross-Domain Macro-F1 Matrices (for reference; diagonals are in-domain)

#### DeiT-Small ‚Äî Train √ó Test (Macro-F1 ‚Üë)
| Train \ Test | 100√ó | 400√ó | Mixed |
|---|---:|---:|---:|
| **100√ó** | 0.979 *(in-domain)* | **0.518** | **0.769** |
| **400√ó** | **0.405** | 0.996 *(in-domain)* | **0.734** |
| **Mixed** | **0.921** | **0.933** | 0.920 *(in-domain)* |

#### Swin-Tiny ‚Äî Train √ó Test (Macro-F1 ‚Üë)
| Train \ Test | 100√ó | 400√ó | Mixed |
|---|---:|---:|---:|
| **100√ó** | 0.958 *(in-domain)* | **0.337** | **0.681** |
| **400√ó** | **0.245** | 0.992 *(in-domain)* | **0.685** |
| **Mixed** | **0.955** | **0.963** | 0.963 *(in-domain)* |

> **Read:** focus on the **bold off-diagonal** cells‚Äîthose are the cross-domain results.

![In-Domain vs Cross-Domain (use the orange bars)](path/to/indomain_vs_crossdomain.png)  
<sub><b>Figure 3.</b> Averages of **in-domain** (blue, diagonal) vs **cross-domain** (orange, off-diagonal). For this section, focus on **cross-domain (orange)** to gauge robustness.</sub>

---

### üìê Directional Asymmetry (train ‚Üí test)
| Model | 100√ó ‚Üí 400√ó | 400√ó ‚Üí 100√ó |
|---|---:|---:|
| **DeiT-Small** | **0.518** | **0.405** |
| **Swin-Tiny**  | **0.337** | **0.245** |

**Interpretation:** **400√ó ‚Üí 100√ó** consistently underperforms **100√ó ‚Üí 400√ó**, suggesting models trained on high-mag textures struggle to generalize down to lower magnification.

![Directional Generalization Asymmetry](path/to/directional_generalization.png)  
<sub><b>Figure 2.</b> **Directional gap** between 100√ó‚Üí400√ó and 400√ó‚Üí100√ó. Both models struggle more when moving **down** in magnification (400√ó‚Üí100√ó).</sub>

---

### üß™ Generalization from **Mixed** Training (cross-domain only)
| Model | Mixed ‚Üí 100√ó | Mixed ‚Üí 400√ó |
|---|---:|---:|
| **DeiT-Small** | **0.921** | **0.933** |
| **Swin-Tiny**  | **0.955** | **0.963** |

**Interpretation:** Mixed training substantially reduces domain shift‚Äî**Swin-Tiny (Mixed)** is the most robust, with near-symmetric performance to both 100√ó and 400√ó.

![Generalization from Mixed Training](path/to/generalization_from_mixed.png)  
<sub><b>Figure 1.</b> Cross-domain generalization from **Mixed training**. Bars show Macro-F1 on 100√ó and 400√ó tests (ignore Mixed‚ÜíMixed as it‚Äôs in-domain). **Swin-Tiny (Mixed)** is strongest and most symmetric.</sub>

---

### üßÆ Cross-Domain Mean (average of all off-diagonal cells)
| Model | Mean Macro-F1 (‚Üë) |
|---|---:|
| **DeiT-Small** | **0.713** |
| **Swin-Tiny**  | **0.644** |

**Interpretation:** On average across all cross-domain conditions, **DeiT** edges **Swin**‚Äîbut **Swin-Tiny (Mixed)** is the **best single recipe** if you can only train once and must handle both magnifications at test time.

![Cross-Domain Robustness Heatmaps](path/to/crossdomain_robustness.png)  
<sub><b>Figure 4.</b> Heatmaps of Train√óTest Macro-F1. Emphasize the **off-diagonal** cells. Mixed rows are uniformly high, especially for **Swin-Tiny**.</sub>

---

## üñºÔ∏è Figures (cross-domain focus)

![Generalization from Mixed Training](path/to/generalization_from_mixed.png)  
<sub><b>Figure 1.</b> Cross-domain generalization from **Mixed training**. Bars show Macro-F1 on 100√ó and 400√ó tests (ignore Mixed‚ÜíMixed as it‚Äôs in-domain). **Swin-Tiny (Mixed)** is strongest and most symmetric.</sub>

![Directional Generalization Asymmetry](path/to/directional_generalization.png)  
<sub><b>Figure 2.</b> **Directional gap** between 100√ó‚Üí400√ó and 400√ó‚Üí100√ó. Both models struggle more when moving **down** in magnification (400√ó‚Üí100√ó).</sub>

![In-Domain vs Cross-Domain (use the orange bars)](path/to/indomain_vs_crossdomain.png)  
<sub><b>Figure 3.</b> Averages of **in-domain** (blue, diagonal) vs **cross-domain** (orange, off-diagonal). For this section, focus on **cross-domain (orange)** to gauge robustness.</sub>

![Cross-Domain Robustness Heatmaps](path/to/crossdomain_robustness.png)  
<sub><b>Figure 4.</b> Heatmaps of Train√óTest Macro-F1. Emphasize the **off-diagonal** cells. Mixed rows are uniformly high, especially for **Swin-Tiny**.</sub>

---

## ‚è±Ô∏è Efficiency Snapshot ‚Äî In-Domain Only
*Compare speed/memory where each model is evaluated on its training distribution.*

| Model | Train=Test | Latency (ms/img) (‚Üì) | Throughput (img/s) (‚Üë) | Peak GPU (MB) |
|---|---|---:|---:|---:|
| **DeiT-Small** | 100√ó | **166.581** | **6.003** | **528.254** |
| **DeiT-Small** | 400√ó | 704.081 | 1.420 | 528.254 |
| **DeiT-Small** | Mixed | 477.284 | 2.095 | 528.004 |
| **Swin-Tiny** | 100√ó | 303.168 | 3.298 | 1302.659 |
| **Swin-Tiny** | 400√ó | 269.120 | 3.716 | 1302.659 |
| **Swin-Tiny** | Mixed | 224.129 | 4.462 | 1302.659 |

**What it shows:** DeiT has **lower VRAM** footprint and is **fastest** on **100√ó** in-domain; Swin is fastest on **Mixed** and **400√ó** among its own runs.

---

## üß© Optional: Subgroup Summary (Macro-F1, Acc, BalAcc) ‚Äî Per Test Distribution
*Handy for quick graphing across test sets; combine rows from the three runs of each model.*

### Swin-Tiny ‚Äî by Test Distribution
| Test Group | Macro-F1 | Acc | BalAcc | Latency (ms/img) | Throughput (img/s) |
|---|---:|---:|---:|---:|---:|
| **100√ó** (from 100√ó run) | 0.958 | 0.958 | 0.958 | 303.168 | 3.298 |
| **400√ó** (from 400√ó run) | 0.992 | 0.992 | 0.992 | 269.120 | 3.716 |
| **Mixed** (from Mixed run) | 0.963 | 0.963 | 0.963 | 224.129 | 4.462 |

### DeiT-Small ‚Äî by Test Distribution
| Test Group | Macro-F1 | Acc | BalAcc | Latency (ms/img) | Throughput (img/s) |
|---|---:|---:|---:|---:|---:|
| **100√ó** (from 100√ó run) | 0.979 | 0.979 | 0.979 | 166.581 | 6.003 |
| **400√ó** (from 400√ó run) | 0.996 | 0.996 | 0.996 | 704.081 | 1.420 |
| **Mixed** (from Mixed run) | 0.920 | 0.921 | 0.921 | 477.284 | 2.095 |

**What it shows:** a compact ‚Äúbest per test distribution‚Äù view, useful for **bar charts** (Macro-F1, Acc, BalAcc) and **runtime plots**.

---

### ‚úÖ Notes for plotting
- Use **Macro-F1** as the headline bar; overlay **Acc**/**BalAcc** if needed.
- For **robustness**, heatmap the **Cross-Domain** matrices.
- For **efficiency**, draw **latency vs throughput** scatter per model/setup.



---

## üìö **Citations (Background)**

- **DeiT:** Touvron et al., *Training data-efficient image transformers & distillation through attention*. ICML 2021.  
- **Swin Transformer:** Liu et al., *Swin Transformer: Hierarchical Vision Transformer using Shifted Windows*. 2021.  
- **BreakHis:** Spanhol et al., *A Dataset for Breast Cancer Histopathological Image Classification*. IEEE T-BioCAS 2016.

*(Please cite the original papers and the dataset per their licenses.)*

---

## üìú **License**

Released under the **MIT License**. See [`LICENSE`](LICENSE) for details.
