{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 🔧 Library Imports\n",
        "\n",
        "We begin by importing essential libraries required for training a CycleGAN using PyTorch. These include modules for neural network construction (`torch.nn`), optimization (`torch.optim`), image preprocessing (`torchvision.transforms`), dataset loading, and image manipulation via PIL. We also import helper modules for dynamic computation graphs and data iteration.\n"
      ],
      "metadata": {
        "id": "byH3i-uZJIy7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYyMs2Gbjn3H"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "import os\n",
        "import itertools\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧹 Clear CUDA Cache\n",
        "\n",
        "Before training begins, we clear the CUDA memory cache to prevent potential memory issues on the GPU.\n"
      ],
      "metadata": {
        "id": "KkFrx1mXJKGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear CUDA cache\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "Ck8q88Mspaed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📁 Mount Google Drive\n",
        "\n",
        "To access the dataset stored in Google Drive, we mount the drive into the Colab runtime. This allows for persistent access to image folders and model checkpoints.\n"
      ],
      "metadata": {
        "id": "jAERLOF8JNYm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScRJ-64Ij-Yw"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📥 Data Loading\n",
        "\n",
        "We begin by loading the benign and malignant datasets using `ImageFolder`. Each image is transformed with:\n",
        "- `ToTensor()` – converts the image to a PyTorch tensor\n",
        "- `Resize(224x224)` – resizes the image to match input requirements of VGG16\n",
        "- `Normalize` – scales pixel values to [-1, 1]\n",
        "\n",
        "Each dataset is loaded separately and wrapped in a `DataLoader` for iteration during training.\n"
      ],
      "metadata": {
        "id": "ZnNshvwlIcQw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5rM2akcj-Wa"
      },
      "outputs": [],
      "source": [
        "# Load Data\n",
        "#Sequence of transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((224, 224)),  # Resize images to 224x224 pixels\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "new_size = 1 #batch size\n",
        "dataset_A = datasets.ImageFolder(root='/content/drive/MyDrive/my_data/train/benign', transform=transform) #load benign dataset\n",
        "dataset_B = datasets.ImageFolder(root='/content/drive/MyDrive/my_data/train/malignant', transform=transform) #load malignant dataset\n",
        "#Data loader for each dataset\n",
        "loader_A = torch.utils.data.DataLoader(dataset_A, batch_size=new_size, shuffle=True)\n",
        "loader_B = torch.utils.data.DataLoader(dataset_B, batch_size=new_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🖼️ Visualization\n",
        "\n",
        "Before training the model, we visualize one image from each class to confirm proper data loading and transformations.\n",
        "\n",
        "This step helps ensure that our input pipeline is functioning correctly and the images are in the expected format.\n"
      ],
      "metadata": {
        "id": "pLISbKohIeJ3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDAqrnYBj-TA"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def imshow(img):\n",
        "    img = img.numpy().transpose((1, 2, 0))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "\n",
        "dataiter_A = iter(loader_A)\n",
        "images_A, _ = next(dataiter_A)\n",
        "\n",
        "dataiter_B = iter(loader_B)\n",
        "images_B, _ = next(dataiter_B)\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Sample from benign ()')\n",
        "imshow(images_A[0])\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Sample from malignant ()')\n",
        "imshow(images_B[0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧠 Generator Definition (ResNet-Based)\n",
        "\n",
        "We define the generator using a ResNet-based architecture. It includes:\n",
        "- Initial convolutional layer with reflection padding\n",
        "- Two downsampling layers (Conv + ReLU)\n",
        "- A configurable number of ResNet blocks\n",
        "- Two upsampling layers\n",
        "- Final output layer with `tanh` activation to scale output to [-1, 1]\n",
        "\n",
        "This generator learns to translate images from one domain to another while preserving structure.\n"
      ],
      "metadata": {
        "id": "b-PjcqlBIgrf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🔁 ResNet Block (Used in Generator)\n",
        "\n",
        "Each ResNet block uses:\n",
        "- Two convolutional layers with normalization and ReLU\n",
        "- Optional dropout for regularization\n",
        "- A skip connection to preserve identity mapping\n",
        "\n",
        "These blocks are stacked to increase the generator’s capacity while maintaining stable training dynamics.\n"
      ],
      "metadata": {
        "id": "MMaGekuNIncv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWCl711aj-Q7"
      },
      "outputs": [],
      "source": [
        "#######Generator##########\n",
        "#########################\n",
        "\n",
        "class ResnetGenerator(nn.Module):\n",
        "    def __init__(self, input_nc, output_nc, n_blocks=9, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
        "        assert(n_blocks >= 0)\n",
        "        super(ResnetGenerator, self).__init__()\n",
        "        self.input_nc = input_nc\n",
        "        self.output_nc = output_nc\n",
        "        self.ngf = ngf #no of generator filters\n",
        "        #n_blocks = resnet blocks\n",
        "\n",
        "        #Initial convlutional block\n",
        "        model = [nn.ReflectionPad2d(3),\n",
        "                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0, bias=True),\n",
        "                 norm_layer(ngf),\n",
        "                 nn.ReLU(True)]\n",
        "\n",
        "        # Downsample\n",
        "        #reducing spatial dimensions\n",
        "        n_downsampling = 2 #no. of downsampling layers\n",
        "        for i in range(n_downsampling):\n",
        "            mult = 2**i\n",
        "            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1, bias=True),\n",
        "                      norm_layer(ngf * mult * 2),\n",
        "                      nn.ReLU(True)]\n",
        "\n",
        "        # Resnet blocks\n",
        "        #using shortcut connections bypassing few layers\n",
        "        mult = 2**n_downsampling\n",
        "        for i in range(n_blocks):\n",
        "            model += [ResnetBlock(ngf * mult, padding_type='reflect', norm_layer=norm_layer, use_dropout=use_dropout, use_bias=True)]\n",
        "\n",
        "        # Upsample\n",
        "        for i in range(n_downsampling):\n",
        "            mult = 2**(n_downsampling - i)\n",
        "            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n",
        "                                         kernel_size=3, stride=2,\n",
        "                                         padding=1, output_padding=1,\n",
        "                                         bias=True),\n",
        "                      norm_layer(int(ngf * mult / 2)),\n",
        "                      nn.ReLU(True)]\n",
        "\n",
        "        model += [nn.ReflectionPad2d(3)]\n",
        "        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n",
        "        model += [nn.Tanh()]\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.model(input)\n",
        "#resnet block\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
        "        super(ResnetBlock, self).__init__()\n",
        "        # Create the convolutional block\n",
        "        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)\n",
        "    #function to build convolution block\n",
        "    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
        "        conv_block = [] #to hold layers\n",
        "        p = 0\n",
        "\n",
        "        #determining padding type for first layer\n",
        "        if padding_type == 'reflect':\n",
        "            conv_block += [nn.ReflectionPad2d(1)]\n",
        "        elif padding_type == 'replicate':\n",
        "            conv_block += [nn.ReplicationPad2d(1)]\n",
        "        else:\n",
        "            p = 1  # 'zero' padding\n",
        "        #first convolution layer\n",
        "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n",
        "                       norm_layer(dim),\n",
        "                       nn.ReLU(True)]\n",
        "        #opyional dropout layer\n",
        "        if use_dropout:\n",
        "            conv_block += [nn.Dropout(0.5)]\n",
        "\n",
        "        #determining padding type for 2nd layer\n",
        "        p = 0\n",
        "        if padding_type == 'reflect':\n",
        "            conv_block += [nn.ReflectionPad2d(1)]\n",
        "        elif padding_type == 'replicate':\n",
        "            conv_block += [nn.ReplicationPad2d(1)]\n",
        "        else:\n",
        "            p = 1  # 'zero' padding\n",
        "        #second convolutional block\n",
        "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n",
        "                       norm_layer(dim)]\n",
        "\n",
        "        return nn.Sequential(*conv_block)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.conv_block(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧪 Discriminator Definition (VGG16-Based)\n",
        "\n",
        "We define a custom discriminator built on top of a pre-trained VGG16 model. It performs:\n",
        "- **Fake/Real classification** to guide adversarial training\n",
        "- **Benign/Malignant classification** for domain-specific output\n",
        "\n",
        "The VGG16 features are used as input to two separate fully connected heads for dual objectives.\n"
      ],
      "metadata": {
        "id": "CzWGYIeAIklo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9lR6bb2j-NU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class CustomDiscriminator(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(CustomDiscriminator, self).__init__()\n",
        "        # Load the pre-trained VGG16 model\n",
        "        vgg16 = models.vgg16(pretrained=True)\n",
        "\n",
        "        # Remove the classifier part of VGG16\n",
        "        self.features = vgg16.features\n",
        "\n",
        "        # Calculate the size of the feature map after VGG16 features\n",
        "        # Assuming input image size of (3, 224, 224)\n",
        "        self.feature_map_size = 512 * 7 * 7\n",
        "\n",
        "        # Define the classifier for fake/real\n",
        "        self.fake_real_classifier = nn.Sequential(\n",
        "            nn.Linear(self.feature_map_size, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 1),  # Binary classification (fake/real)\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Define the classifier for benign/malignant\n",
        "        self.benign_malignant_classifier = nn.Sequential(\n",
        "            nn.Linear(self.feature_map_size, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, num_classes)  # Multi-class classification (benign/malignant)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract features using VGG16\n",
        "        features = self.features(x)\n",
        "        features = features.view(features.size(0), -1)  # Flatten the feature map\n",
        "\n",
        "        # Fake/Real classification\n",
        "        fake_real_output = self.fake_real_classifier(features)\n",
        "\n",
        "        # Benign/Malignant classification\n",
        "        benign_malignant_output = self.benign_malignant_classifier(features)\n",
        "\n",
        "        return fake_real_output, benign_malignant_output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 💻 Select Compute Device\n",
        "\n",
        "We define the computation device, defaulting to GPU (`cuda`) if available, otherwise falling back to CPU. This enables seamless training on Colab’s hardware accelerators.\n"
      ],
      "metadata": {
        "id": "FtJ3SHezJWWB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRPBPMWRj-Lp"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⚙️ Hyperparameters\n",
        "\n",
        "We set the basic hyperparameters for our CycleGAN:\n",
        "- `input_nc`: number of input channels (3 for RGB)\n",
        "- `output_nc`: number of output channels (3 for RGB)\n",
        "- `n_residual_blocks`: number of ResNet blocks in the generator\n",
        "- `lr`, `beta1`: learning rate and momentum term for Adam optimizer\n"
      ],
      "metadata": {
        "id": "ec74dd4OIutB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7K-UKjhko7U"
      },
      "outputs": [],
      "source": [
        "input_nc = 3  # number of channels in the input images\n",
        "output_nc = 3  # number of channels in the output images\n",
        "n_residual_blocks = 9  # typical number for a CycleGAN\n",
        "\n",
        "lr = 0.0002\n",
        "beta1 = 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧰 Model Initialization\n",
        "\n",
        "We initialize:\n",
        "- Two generators: `netG_A2B` (Benign → Malignant) and `netG_B2A` (Malignant → Benign)\n",
        "- Two discriminators: `D_A` and `D_B` for each domain\n",
        "\n",
        "All models are moved to the selected `device` (CPU/GPU).\n"
      ],
      "metadata": {
        "id": "MHasYY5GIxzP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QeQYcDxko49"
      },
      "outputs": [],
      "source": [
        "# Generators\n",
        "netG_A2B = ResnetGenerator(input_nc, output_nc, n_blocks=n_residual_blocks).to(device)\n",
        "netG_B2A = ResnetGenerator(input_nc, output_nc, n_blocks=n_residual_blocks).to(device)\n",
        "\n",
        "# Discriminators\n",
        "D_A = CustomDiscriminator().to(device)\n",
        "D_B = CustomDiscriminator().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🚀 Optimizers\n",
        "\n",
        "We define optimizers for:\n",
        "- Generators (`netG_A2B`, `netG_B2A`) using a single `Adam` optimizer\n",
        "- Each discriminator (`D_A`, `D_B`) using separate `Adam` optimizers\n",
        "\n",
        "Learning rate and beta values are consistent across optimizers.\n"
      ],
      "metadata": {
        "id": "nrupg595Iy_g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XCPjHALko1d"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "# Optimizers\n",
        "# Define optimizers\n",
        "optimizer_G = optim.Adam(itertools.chain(netG_A2B.parameters(), netG_B2A.parameters()), lr=lr, betas=(beta1, 0.999))\n",
        "optimizer_D_A = optim.Adam(D_A.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optimizer_D_B = optim.Adam(D_B.parameters(), lr=lr, betas=(beta1, 0.999))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📉 Loss Functions\n",
        "\n",
        "We use the following loss functions:\n",
        "- `MSELoss` for adversarial GAN loss\n",
        "- `L1Loss` for cycle-consistency and identity loss\n",
        "- `CrossEntropyLoss` for benign/malignant classification\n",
        "\n",
        "All losses are computed on GPU if available.\n"
      ],
      "metadata": {
        "id": "AtkZm18yI1UX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ducpFdhbkozb"
      },
      "outputs": [],
      "source": [
        "# Define loss functions\n",
        "criterion_GAN = nn.MSELoss().to(device)\n",
        "criterion_cycle = nn.L1Loss().to(device)\n",
        "criterion_identity = nn.L1Loss().to(device)\n",
        "criterion_classification = nn.CrossEntropyLoss().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📊 Visualization Function\n",
        "\n",
        "This utility function visualizes a **real** and **generated (fake)** image side-by-side for comparison.\n",
        "\n",
        "It uses `matplotlib` and `torchvision.utils.make_grid` to properly format and normalize tensors for display.\n"
      ],
      "metadata": {
        "id": "zRl7VfVuI4K1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2a7k_66mkouR"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "def plot_single_real_and_fake_image(real_image, fake_image):\n",
        "    \"\"\"\n",
        "    Plots a comparison of a single real and a single generated (fake) image.\n",
        "\n",
        "    Parameters:\n",
        "    - real_image: a single Tensor image (C, H, W).\n",
        "    - fake_image: a single Tensor image (C, H, W).\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8, 4))\n",
        "\n",
        "    # Display the real image\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Real Image\")\n",
        "    real_image = vutils.make_grid(real_image, normalize=True).permute(1, 2, 0).cpu().numpy()\n",
        "    plt.imshow(real_image)\n",
        "\n",
        "    # Display the fake image\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Generated Image\")\n",
        "    fake_image = vutils.make_grid(fake_image, normalize=True).permute(1, 2, 0).cpu().numpy()\n",
        "    plt.imshow(fake_image)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 💾 Save Model to Drive\n",
        "\n",
        "This helper function saves all four models (`netG_A2B`, `netG_B2A`, `D_A`, `D_B`) to a specified Google Drive folder for checkpointing.\n",
        "\n",
        "Each model is saved with the epoch number in its filename, and the folder is created if it doesn’t exist.\n"
      ],
      "metadata": {
        "id": "ERYmbfB0I6-W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKYb3fgBkoqx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "def save_models_to_drive(epoch, netG_A2B, netG_B2A, D_A, D_B, drive_path='/content/drive/MyDrive/vgg_16_CycleGAN_Models'):\n",
        "    \"\"\"\n",
        "    Save model parameters to Google Drive.\n",
        "\n",
        "    Parameters:\n",
        "        epoch (int): The current epoch number.\n",
        "        netG_A2B (nn.Module): Generator model from domain A to B.\n",
        "        netG_B2A (nn.Module): Generator model from domain B to A.\n",
        "        netD_A (nn.Module): Discriminator model for domain A.\n",
        "        netD_B (nn.Module): Discriminator model for domain B.\n",
        "        drive_path (str): The path in Google Drive to save the models.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(drive_path):\n",
        "        os.makedirs(drive_path)\n",
        "\n",
        "    # Define file paths for saving\n",
        "    path_G_A2B = os.path.join(drive_path, f'netG_A2B_epoch_{epoch}.pth')\n",
        "    path_G_B2A = os.path.join(drive_path, f'netG_B2A_epoch_{epoch}.pth')\n",
        "    path_D_A = os.path.join(drive_path, f'netD_A_epoch_{epoch}.pth')\n",
        "    path_D_B = os.path.join(drive_path, f'netD_B_epoch_{epoch}.pth')\n",
        "\n",
        "    # Save the models\n",
        "    torch.save(netG_A2B.state_dict(), path_G_A2B)\n",
        "    torch.save(netG_B2A.state_dict(), path_G_B2A)\n",
        "    torch.save(D_A.state_dict(), path_D_A)\n",
        "    torch.save(D_B.state_dict(), path_D_B)\n",
        "\n",
        "    print(f\"Saved models at epoch {epoch} to {drive_path}\")\n",
        "\n",
        "# Example usage within the training loop:\n",
        "# save_models_to_drive(epoch, netG_A2B, netG_B2A, netD_A, netD_B)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear CUDA cache\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "QL5AtDnvpq2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🏋️ Training Loop\n",
        "\n",
        "The core training loop includes:\n",
        "- Forward pass through both generators and discriminators\n",
        "- Calculation of identity, GAN, cycle-consistency, and classification losses\n",
        "- Backpropagation and optimization for all networks\n",
        "- Logging of progress and sample visualizations every few batches\n",
        "\n",
        "At the end of each epoch, models are optionally saved to Google Drive.\n",
        "\n",
        "This loop trains the CycleGAN to translate between benign and malignant domains while maintaining visual realism and class consistency.\n"
      ],
      "metadata": {
        "id": "3u1-aqTDI_cQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⏱️ Total Training Time\n",
        "\n",
        "We calculate and print the total training time once all epochs are completed. This gives an idea of the training cost for this setup.\n"
      ],
      "metadata": {
        "id": "dtAvifH1JAcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 25\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.autograd import Variable\n",
        "import itertools\n",
        "import time\n",
        "\n",
        "import os\n",
        "\n",
        "# Record the total training start time\n",
        "total_training_start_time = time.time()\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    # Record the start time of the epoch\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    for i, (real_A, real_B) in enumerate(zip(loader_A, loader_B)):\n",
        "        # Set model input\n",
        "        real_A = Variable(real_A[0].to(device)) # moving images from domain A to CUDA\n",
        "        real_B = Variable(real_B[0].to(device)) # moving images from domain B to CUDA\n",
        "\n",
        "        # -------------------------------\n",
        "        #  Train Generators A2B and B2A\n",
        "        # -------------------------------\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # Identity loss\n",
        "        loss_id_A = criterion_identity(netG_B2A(real_A), real_A)\n",
        "        loss_id_B = criterion_identity(netG_A2B(real_B), real_B)\n",
        "\n",
        "        # GAN loss\n",
        "        fake_B = netG_A2B(real_A) # generating images from A to B domain\n",
        "        pred_fake, class_fake_B = D_B(fake_B)  # predicting real/fake and class\n",
        "        loss_GAN_A2B = criterion_GAN(pred_fake, torch.ones(pred_fake.size(), device=device))\n",
        "\n",
        "        fake_A = netG_B2A(real_B) # generating images from B to A domain\n",
        "        pred_fake, class_fake_A = D_A(fake_A)  # predicting real/fake and class\n",
        "        loss_GAN_B2A = criterion_GAN(pred_fake, torch.ones(pred_fake.size(), device=device))\n",
        "\n",
        "        # Cycle loss\n",
        "        recovered_A = netG_B2A(fake_B)\n",
        "        loss_cycle_ABA = criterion_cycle(recovered_A, real_A)\n",
        "\n",
        "        recovered_B = netG_A2B(fake_A)\n",
        "        loss_cycle_BAB = criterion_cycle(recovered_B, real_B)\n",
        "\n",
        "        # Classification loss for fake images (benign/malignant)\n",
        "        target_fake_B = torch.full((class_fake_B.size(0),), 1, device=device, dtype=torch.long)  # All fake_B are malignant (label 1)\n",
        "        target_fake_A = torch.full((class_fake_A.size(0),), 0, device=device, dtype=torch.long)  # All fake_A are benign (label 0)\n",
        "        loss_class_fake_B = criterion_classification(class_fake_B, target_fake_B)\n",
        "        loss_class_fake_A = criterion_classification(class_fake_A, target_fake_A)\n",
        "\n",
        "        # Total loss for Generators\n",
        "        loss_G = (loss_id_A + loss_id_B + loss_GAN_A2B + loss_GAN_B2A +\n",
        "                  loss_cycle_ABA + loss_cycle_BAB + loss_class_fake_B + loss_class_fake_A)\n",
        "        loss_G.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # -----------------------\n",
        "        #  Train Discriminator D_A\n",
        "        # -----------------------\n",
        "        optimizer_D_A.zero_grad()\n",
        "\n",
        "        # Real loss\n",
        "        pred_real_A, class_real_A = D_A(real_A)\n",
        "        loss_D_real_A = criterion_GAN(pred_real_A, torch.ones(pred_real_A.size(), device=device))\n",
        "        target_real_A = torch.full((class_real_A.size(0),), 0, device=device, dtype=torch.long)  # All real_A are benign (label 0)\n",
        "        loss_class_real_A = criterion_classification(class_real_A, target_real_A)\n",
        "\n",
        "        # Fake loss (detach to avoid training G on these labels)\n",
        "        pred_fake_A, class_fake_A = D_A(fake_A.detach())\n",
        "        loss_D_fake_A = criterion_GAN(pred_fake_A, torch.zeros(pred_fake_A.size(), device=device))\n",
        "        loss_class_fake_A = criterion_classification(class_fake_A, target_fake_A)\n",
        "\n",
        "        # Total loss for Discriminator A\n",
        "        loss_D_A = (loss_D_real_A + loss_D_fake_A + loss_class_real_A + loss_class_fake_A) / 2\n",
        "        loss_D_A.backward()\n",
        "        optimizer_D_A.step()\n",
        "\n",
        "        # -----------------------\n",
        "        #  Train Discriminator D_B\n",
        "        # -----------------------\n",
        "        optimizer_D_B.zero_grad()\n",
        "\n",
        "        # Real loss\n",
        "        pred_real_B, class_real_B = D_B(real_B)\n",
        "        loss_D_real_B = criterion_GAN(pred_real_B, torch.ones(pred_real_B.size(), device=device))\n",
        "        target_real_B = torch.full((class_real_B.size(0),), 1, device=device, dtype=torch.long)  # All real_B are malignant (label 1)\n",
        "        loss_class_real_B = criterion_classification(class_real_B, target_real_B)\n",
        "\n",
        "        # Fake loss (detach to avoid training G on these labels)\n",
        "        pred_fake_B, class_fake_B = D_B(fake_B.detach())\n",
        "        loss_D_fake_B = criterion_GAN(pred_fake_B, torch.zeros(pred_fake_B.size(), device=device))\n",
        "        loss_class_fake_B = criterion_classification(class_fake_B, target_fake_B)\n",
        "\n",
        "        # Total loss for Discriminator B\n",
        "        loss_D_B = (loss_D_real_B + loss_D_fake_B + loss_class_real_B + loss_class_fake_B) / 2\n",
        "        loss_D_B.backward()\n",
        "        optimizer_D_B.step()\n",
        "\n",
        "        # ---------------------\n",
        "        #  Log Progress\n",
        "        # ---------------------\n",
        "        print(f\"Epoch [{epoch}/{num_epochs}] Batch {i}/{len(loader_A)} \\\n",
        "              Loss D_A: {loss_D_A.item()}, Loss D_B: {loss_D_B.item()} \\\n",
        "              Loss G: {loss_G.item()}\")\n",
        "\n",
        "        # If at save interval => save generated image samples\n",
        "        if i % 20 == 0:  # For example, visualize every 20 batches\n",
        "            plot_single_real_and_fake_image(real_A[0], fake_B[0])  # Pass the first image of the batch\n",
        "            plot_single_real_and_fake_image(real_B[0], fake_A[0])\n",
        "\n",
        "            # Predict class of images and print\n",
        "            _, class_real_A = D_A(real_A)\n",
        "            _, class_real_B = D_B(real_B)\n",
        "            _, class_fake_A = D_A(fake_A)\n",
        "            _, class_fake_B = D_B(fake_B)\n",
        "\n",
        "            pred_class_real_A = class_real_A.argmax(dim=1).item()\n",
        "            pred_class_real_B = class_real_B.argmax(dim=1).item()\n",
        "            pred_class_fake_A = class_fake_A.argmax(dim=1).item()\n",
        "            pred_class_fake_B = class_fake_B.argmax(dim=1).item()\n",
        "\n",
        "            print(f\"Predicted class for real_A: {'Benign' if pred_class_real_A == 0 else 'Malignant'}\")\n",
        "            print(f\"Predicted class for real_B: {'Benign' if pred_class_real_B == 0 else 'Malignant'}\")\n",
        "            print(f\"Predicted class for fake_A: {'Benign' if pred_class_fake_A == 0 else 'Malignant'}\")\n",
        "            print(f\"Predicted class for fake_B: {'Benign' if pred_class_fake_B == 0 else 'Malignant'}\")\n",
        "\n",
        "    # Record the end time of the epoch\n",
        "    epoch_end_time = time.time()\n",
        "    epoch_duration = epoch_end_time - epoch_start_time\n",
        "    print(f\"Epoch {epoch} completed in {epoch_duration:.2f} seconds\")\n",
        "\n",
        "    # Update learning rates\n",
        "    #lr_scheduler_G.step()\n",
        "    #lr_scheduler_D_A.step()\n",
        "    #lr_scheduler_D_B.step()\n",
        "\n",
        "    if (epoch + 1) % 1 == 0:  # Every 10 epochs\n",
        "       save_models_to_drive(epoch, netG_A2B, netG_B2A, D_A, D_B)\n",
        "\n",
        "# After the final epoch, save the refined generated images\n",
        "#save_final_generated_images(G, dataloader, classifier, epoch=num_epochs, base_directory=\"output_breakhis/final_images\", device=device)\n",
        "\n",
        "# Record the total training end time\n",
        "total_training_end_time = time.time()\n",
        "total_training_duration = total_training_end_time - total_training_start_time\n",
        "print(f\"Total training time: {total_training_duration:.2f} seconds\")"
      ],
      "metadata": {
        "id": "7pxg7TKhcpri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KPdNc2d3iW29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eJ72yc5yusXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XYf3PsbviWvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKKeB1o5mDxv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}