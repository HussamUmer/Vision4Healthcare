{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 🔧 Library Imports\n",
        "\n",
        "We begin by importing essential libraries required for training a CycleGAN using PyTorch. These include modules for neural network construction (`torch.nn`), optimization (`torch.optim`), image preprocessing (`torchvision.transforms`), dataset loading, and image manipulation via PIL. We also import helper modules for dynamic computation graphs and data iteration.\n"
      ],
      "metadata": {
        "id": "ulFB7oO2CsGG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65vhnvhFSnih"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "import os\n",
        "import itertools\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📁 Mount Google Drive\n",
        "\n",
        "To access the dataset stored in Google Drive, we mount the drive into the Colab runtime. This allows for persistent access to image folders and model checkpoints.\n"
      ],
      "metadata": {
        "id": "vzHEQwTmCwuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "FGPNMsMeTuES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📁Load and Preprocess the Datasets\n",
        "\n",
        "In this step, we load two image datasets — one for **benign** and one for **malignant** cases — directly from Google Drive.  \n",
        "We apply a basic transformation pipeline using `transforms.Compose` that:\n",
        "- Converts each image into a PyTorch tensor\n",
        "- Normalizes the pixel values to have a mean and standard deviation of 0.5 for each channel, scaling them into the range `[-1, 1]`\n",
        "\n",
        "Finally, we wrap the datasets into `DataLoader` objects for batch-wise access during training.\n"
      ],
      "metadata": {
        "id": "Z3MA_bboFPbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data\n",
        "#Sequence of transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "new_size = 1 #batch size\n",
        "dataset_A = datasets.ImageFolder(root='/content/drive/MyDrive/my_data/train/benign', transform=transform) #load benign dataset\n",
        "dataset_B = datasets.ImageFolder(root='/content/drive/MyDrive/my_data/train/malignant', transform=transform) #load malignant dataset\n",
        "#Data loader for each dataset\n",
        "loader_A = torch.utils.data.DataLoader(dataset_A, batch_size=new_size, shuffle=True)\n",
        "loader_B = torch.utils.data.DataLoader(dataset_B, batch_size=new_size, shuffle=True)"
      ],
      "metadata": {
        "id": "7Ucg_Ay5TuBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🖼️ Visualize Sample Images from Each Class\n",
        "\n",
        "Before we begin training, it’s important to visualize a few images from both classes (benign and malignant) to:\n",
        "- Confirm the data was loaded and preprocessed correctly\n",
        "- Gain an intuitive understanding of the visual patterns our model will try to learn\n",
        "\n",
        "We’ll display one image from each domain using `matplotlib`.\n"
      ],
      "metadata": {
        "id": "yr3zI6dbFTGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def imshow(img):\n",
        "    img = img.numpy().transpose((1, 2, 0))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "\n",
        "dataiter_A = iter(loader_A)\n",
        "images_A, _ = next(dataiter_A)\n",
        "\n",
        "dataiter_B = iter(loader_B)\n",
        "images_B, _ = next(dataiter_B)\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Sample from benign ()')\n",
        "imshow(images_A[0])\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Sample from malignant ()')\n",
        "imshow(images_B[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vWVu4paLTt4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⚙️ Define the ResNet-based Generator\n",
        "\n",
        "This step defines the **generator model** based on a ResNet architecture. The generator takes images from one domain (e.g., benign) and transforms them into the other domain (e.g., malignant).\n",
        "\n",
        "The model includes:\n",
        "- Initial convolution block with reflection padding\n",
        "- Downsampling layers to compress spatial information\n",
        "- A series of ResNet blocks with skip connections\n",
        "- Upsampling layers to restore original image resolution\n",
        "- A final output layer with `Tanh` activation to map pixel values to `[-1, 1]`\n",
        "\n",
        "We also define the reusable ResNet block used within the generator.\n"
      ],
      "metadata": {
        "id": "_exM9zwUFXla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#######Generator##########\n",
        "#########################\n",
        "\n",
        "class ResnetGenerator(nn.Module):\n",
        "    def __init__(self, input_nc, output_nc, n_blocks=9, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
        "        assert(n_blocks >= 0)\n",
        "        super(ResnetGenerator, self).__init__()\n",
        "        self.input_nc = input_nc\n",
        "        self.output_nc = output_nc\n",
        "        self.ngf = ngf #no of generator filters\n",
        "        #n_blocks = resnet blocks\n",
        "\n",
        "        #Initial convlutional block\n",
        "        model = [nn.ReflectionPad2d(3),\n",
        "                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0, bias=True),\n",
        "                 norm_layer(ngf),\n",
        "                 nn.ReLU(True)]\n",
        "\n",
        "        # Downsample\n",
        "        #reducing spatial dimensions\n",
        "        n_downsampling = 2 #no. of downsampling layers\n",
        "        for i in range(n_downsampling):\n",
        "            mult = 2**i\n",
        "            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1, bias=True),\n",
        "                      norm_layer(ngf * mult * 2),\n",
        "                      nn.ReLU(True)]\n",
        "\n",
        "        # Resnet blocks\n",
        "        #using shortcut connections bypassing few layers\n",
        "        mult = 2**n_downsampling\n",
        "        for i in range(n_blocks):\n",
        "            model += [ResnetBlock(ngf * mult, padding_type='reflect', norm_layer=norm_layer, use_dropout=use_dropout, use_bias=True)]\n",
        "\n",
        "        # Upsample\n",
        "        for i in range(n_downsampling):\n",
        "            mult = 2**(n_downsampling - i)\n",
        "            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n",
        "                                         kernel_size=3, stride=2,\n",
        "                                         padding=1, output_padding=1,\n",
        "                                         bias=True),\n",
        "                      norm_layer(int(ngf * mult / 2)),\n",
        "                      nn.ReLU(True)]\n",
        "\n",
        "        model += [nn.ReflectionPad2d(3)]\n",
        "        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n",
        "        model += [nn.Tanh()]\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.model(input)\n",
        "#resnet block\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
        "        super(ResnetBlock, self).__init__()\n",
        "        # Create the convolutional block\n",
        "        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)\n",
        "    #function to build convolution block\n",
        "    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
        "        conv_block = [] #to hold layers\n",
        "        p = 0\n",
        "\n",
        "        #determining padding type for first layer\n",
        "        if padding_type == 'reflect':\n",
        "            conv_block += [nn.ReflectionPad2d(1)]\n",
        "        elif padding_type == 'replicate':\n",
        "            conv_block += [nn.ReplicationPad2d(1)]\n",
        "        else:\n",
        "            p = 1  # 'zero' padding\n",
        "        #first convolution layer\n",
        "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n",
        "                       norm_layer(dim),\n",
        "                       nn.ReLU(True)]\n",
        "        #opyional dropout layer\n",
        "        if use_dropout:\n",
        "            conv_block += [nn.Dropout(0.5)]\n",
        "\n",
        "        #determining padding type for 2nd layer\n",
        "        p = 0\n",
        "        if padding_type == 'reflect':\n",
        "            conv_block += [nn.ReflectionPad2d(1)]\n",
        "        elif padding_type == 'replicate':\n",
        "            conv_block += [nn.ReplicationPad2d(1)]\n",
        "        else:\n",
        "            p = 1  # 'zero' padding\n",
        "        #second convolutional block\n",
        "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n",
        "                       norm_layer(dim)]\n",
        "\n",
        "        return nn.Sequential(*conv_block)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.conv_block(x)"
      ],
      "metadata": {
        "id": "IE0qxG-wTtyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧠 Define the Dual Output Discriminator (ResNet50)\n",
        "\n",
        "We now define a **dual-purpose discriminator** built on top of a pre-trained ResNet50 model. This discriminator does two things:\n",
        "1. **Discriminates real vs. fake** images (for adversarial training)\n",
        "2. **Classifies images as benign or malignant** (for medical relevance)\n",
        "\n",
        "We remove the final fully connected layer of ResNet50 and replace it with two custom branches:\n",
        "- One for binary real/fake classification\n",
        "- One for benign/malignant classification\n"
      ],
      "metadata": {
        "id": "H21YHJPCFbkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import resnet50, ResNet50_Weights #pre trained resnet 50 with weights\n",
        "class ResNet50DualOutputDiscriminator(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(ResNet50DualOutputDiscriminator, self).__init__()\n",
        "        self.resnet = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
        "        self.resnet.fc = nn.Identity()  # Removing the fully connected layer\n",
        "\n",
        "        # Real/Fake classifier\n",
        "        self.rf_classifier = nn.Sequential(\n",
        "            nn.Linear(2048, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(512, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Benign/Malignant classifier\n",
        "        self.class_classifier = nn.Sequential(\n",
        "            nn.Linear(2048, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.resnet(x)\n",
        "        real_fake_output = self.rf_classifier(features)\n",
        "        class_output = self.class_classifier(features)\n",
        "        return real_fake_output, class_output\n"
      ],
      "metadata": {
        "id": "tDKg8PVPWMW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 💻 Select Compute Device\n",
        "\n",
        "We define the computation device, defaulting to GPU (`cuda`) if available, otherwise falling back to CPU. This enables seamless training on Colab’s hardware accelerators.\n"
      ],
      "metadata": {
        "id": "2ulxQUYPC47p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "YMvbpMtcWMQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🔧 Define Training Hyperparameters\n",
        "\n",
        "Here, we specify the core hyperparameters used across the training pipeline:\n",
        "- `input_nc` and `output_nc`: Number of channels in the input and output images (3 for RGB)\n",
        "- `n_residual_blocks`: Number of ResNet blocks inside the generator\n",
        "- `lr` and `beta1`: Learning rate and momentum term for Adam optimizer\n"
      ],
      "metadata": {
        "id": "Tw4RinTkFgLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_nc = 3  # number of channels in the input images\n",
        "output_nc = 3  # number of channels in the output images\n",
        "n_residual_blocks = 9  # typical number for a CycleGAN\n",
        "\n",
        "lr = 0.0002\n",
        "beta1 = 0.5"
      ],
      "metadata": {
        "id": "1b0zSjjsWMLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🔄 Initialize Generators and Discriminators\n",
        "\n",
        "We now create two generators and two discriminators:\n",
        "- `netG_A2B`: Converts images from domain A (benign) to domain B (malignant)\n",
        "- `netG_B2A`: Converts images from malignant back to benign\n",
        "- `netD_A`: Discriminator for domain A (benign)\n",
        "- `netD_B`: Discriminator for domain B (malignant)\n",
        "\n",
        "These models will be trained adversarially in the CycleGAN setup.\n"
      ],
      "metadata": {
        "id": "BM1QyKIhFkO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generators\n",
        "netG_A2B = ResnetGenerator(input_nc, output_nc, n_blocks=n_residual_blocks).to(device)\n",
        "netG_B2A = ResnetGenerator(input_nc, output_nc, n_blocks=n_residual_blocks).to(device)\n",
        "\n",
        "# Discriminators\n",
        "netD_A = ResNet50DualOutputDiscriminator().to(device)\n",
        "netD_B = ResNet50DualOutputDiscriminator().to(device)"
      ],
      "metadata": {
        "id": "BuaWrVepWY8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🚀 Define Optimizers for All Networks\n",
        "\n",
        "In this step, we define the optimizers that will be used to update the weights of:\n",
        "- Both generators (`optimizer_G`)\n",
        "- Both discriminators (`optimizer_D_A` and `optimizer_D_B`)\n",
        "\n",
        "We use the Adam optimizer with a learning rate of `0.0002` and `beta1=0.5`, which is common for GAN training.\n"
      ],
      "metadata": {
        "id": "FW7CTbhRFocP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "# Optimizers\n",
        "# Define optimizers\n",
        "optimizer_G = optim.Adam(itertools.chain(netG_A2B.parameters(), netG_B2A.parameters()), lr=lr, betas=(beta1, 0.999))\n",
        "optimizer_D_A = optim.Adam(netD_A.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optimizer_D_B = optim.Adam(netD_B.parameters(), lr=lr, betas=(beta1, 0.999))"
      ],
      "metadata": {
        "id": "6CkXzQYtWY3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📉 Define Loss Functions\n",
        "\n",
        "To guide the training process, we define several loss functions:\n",
        "- **Adversarial loss** (`MSELoss`): Encourages realistic image generation\n",
        "- **Cycle-consistency loss** (`L1Loss`): Ensures translation from A → B → A brings back the original image\n",
        "- **Identity loss** (`L1Loss`): Helps maintain color/structure when translating similar images\n",
        "- **Classification loss** (`CrossEntropyLoss`): Ensures the fake images belong to the correct medical class (benign/malignant)\n"
      ],
      "metadata": {
        "id": "p2h9addlFsgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss functions\n",
        "criterion_GAN = nn.MSELoss().to(device)\n",
        "criterion_cycle = nn.L1Loss().to(device)\n",
        "criterion_identity = nn.L1Loss().to(device)\n",
        "criterion_classification = nn.CrossEntropyLoss().to(device)"
      ],
      "metadata": {
        "id": "2lkaGP-DWY0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧪 Define Image Visualization Function\n",
        "\n",
        "This utility function displays a **real image vs. its generated (fake) counterpart** side-by-side using `matplotlib`.\n",
        "\n",
        "It helps monitor the quality of generated images during training and visually assess how well the generator is performing.\n"
      ],
      "metadata": {
        "id": "qJXz3O_KFwh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "def plot_single_real_and_fake_image(real_image, fake_image):\n",
        "    \"\"\"\n",
        "    Plots a comparison of a single real and a single generated (fake) image.\n",
        "\n",
        "    Parameters:\n",
        "    - real_image: a single Tensor image (C, H, W).\n",
        "    - fake_image: a single Tensor image (C, H, W).\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8, 4))\n",
        "\n",
        "    # Display the real image\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Real Image\")\n",
        "    real_image = vutils.make_grid(real_image, normalize=True).permute(1, 2, 0).cpu().numpy()\n",
        "    plt.imshow(real_image)\n",
        "\n",
        "    # Display the fake image\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Generated Image\")\n",
        "    fake_image = vutils.make_grid(fake_image, normalize=True).permute(1, 2, 0).cpu().numpy()\n",
        "    plt.imshow(fake_image)\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "lyt7IRcWWYuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 💾 Save Trained Models to Google Drive\n",
        "\n",
        "To avoid losing progress during training, we define a function that saves the current state of all models (`netG_A2B`, `netG_B2A`, `netD_A`, `netD_B`) to Google Drive.\n",
        "\n",
        "Model checkpoints are saved by epoch, allowing us to resume or analyze training at different stages.\n"
      ],
      "metadata": {
        "id": "UD2bnfN6F0Yf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "def save_models_to_drive(epoch, netG_A2B, netG_B2A, netD_A, netD_B, drive_path='/content/drive/MyDrive/resnet_50_CycleGAN_Models'):\n",
        "    \"\"\"\n",
        "    Save model parameters to Google Drive.\n",
        "\n",
        "    Parameters:\n",
        "        epoch (int): The current epoch number.\n",
        "        netG_A2B (nn.Module): Generator model from domain A to B.\n",
        "        netG_B2A (nn.Module): Generator model from domain B to A.\n",
        "        netD_A (nn.Module): Discriminator model for domain A.\n",
        "        netD_B (nn.Module): Discriminator model for domain B.\n",
        "        drive_path (str): The path in Google Drive to save the models.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(drive_path):\n",
        "        os.makedirs(drive_path)\n",
        "\n",
        "    # Define file paths for saving\n",
        "    path_G_A2B = os.path.join(drive_path, f'netG_A2B_epoch_{epoch}.pth')\n",
        "    path_G_B2A = os.path.join(drive_path, f'netG_B2A_epoch_{epoch}.pth')\n",
        "    path_D_A = os.path.join(drive_path, f'netD_A_epoch_{epoch}.pth')\n",
        "    path_D_B = os.path.join(drive_path, f'netD_B_epoch_{epoch}.pth')\n",
        "\n",
        "    # Save the models\n",
        "    torch.save(netG_A2B.state_dict(), path_G_A2B)\n",
        "    torch.save(netG_B2A.state_dict(), path_G_B2A)\n",
        "    torch.save(netD_A.state_dict(), path_D_A)\n",
        "    torch.save(netD_B.state_dict(), path_D_B)\n",
        "\n",
        "    print(f\"Saved models at epoch {epoch} to {drive_path}\")\n",
        "\n",
        "# Example usage within the training loop:\n",
        "# save_models_to_drive(epoch, netG_A2B, netG_B2A, netD_A, netD_B)\n"
      ],
      "metadata": {
        "id": "Y5K2zmqEWYrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧹 Clear CUDA Cache\n",
        "\n",
        "Before training begins, we clear the CUDA memory cache to prevent potential memory issues on the GPU.\n"
      ],
      "metadata": {
        "id": "yhAjkS1SDAB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear CUDA cache\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "yRD3A5KZ3yzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧠 Train the CycleGAN with Classification\n",
        "\n",
        "This is the main training loop where the entire CycleGAN is trained over multiple epochs.\n",
        "\n",
        "For each epoch and batch:\n",
        "- The generators are trained to fool the discriminators and reconstruct the original images via cycle-consistency\n",
        "- The discriminators are trained to distinguish between real and fake images\n",
        "- Classification loss is also applied to encourage the generated images to have the correct medical label\n",
        "\n",
        "Every few batches, we:\n",
        "- Visualize real vs. fake images\n",
        "- Print out the predicted classes of real and generated samples\n",
        "- Save model checkpoints after each epoch\n",
        "\n",
        "This completes the core CycleGAN training process for benign ↔ malignant image translation.\n"
      ],
      "metadata": {
        "id": "mQsdBxP-F4jG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 25\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.autograd import Variable\n",
        "import itertools\n",
        "import time\n",
        "\n",
        "import os\n",
        "\n",
        "# Record the total training start time\n",
        "total_training_start_time = time.time()\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    # Record the start time of the epoch\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    for i, (real_A, real_B) in enumerate(zip(loader_A, loader_B)):\n",
        "        # Set model input\n",
        "        real_A = Variable(real_A[0].to(device)) # moving images from domain A to CUDA\n",
        "        real_B = Variable(real_B[0].to(device)) # moving images from domain B to CUDA\n",
        "\n",
        "        # -------------------------------\n",
        "        #  Train Generators A2B and B2A\n",
        "        # -------------------------------\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # Identity loss\n",
        "        loss_id_A = criterion_identity(netG_B2A(real_A), real_A)\n",
        "        loss_id_B = criterion_identity(netG_A2B(real_B), real_B)\n",
        "\n",
        "        # GAN loss\n",
        "        fake_B = netG_A2B(real_A) # generating images from A to B domain\n",
        "        pred_fake, class_fake_B = netD_B(fake_B)  # predicting real/fake and class\n",
        "        loss_GAN_A2B = criterion_GAN(pred_fake, torch.ones(pred_fake.size(), device=device))\n",
        "\n",
        "        fake_A = netG_B2A(real_B) # generating images from B to A domain\n",
        "        pred_fake, class_fake_A = netD_A(fake_A)  # predicting real/fake and class\n",
        "        loss_GAN_B2A = criterion_GAN(pred_fake, torch.ones(pred_fake.size(), device=device))\n",
        "\n",
        "        # Cycle loss\n",
        "        recovered_A = netG_B2A(fake_B)\n",
        "        loss_cycle_ABA = criterion_cycle(recovered_A, real_A)\n",
        "\n",
        "        recovered_B = netG_A2B(fake_A)\n",
        "        loss_cycle_BAB = criterion_cycle(recovered_B, real_B)\n",
        "\n",
        "        # Classification loss for fake images (benign/malignant)\n",
        "        target_fake_B = torch.full((class_fake_B.size(0),), 1, device=device, dtype=torch.long)  # All fake_B are malignant (label 1)\n",
        "        target_fake_A = torch.full((class_fake_A.size(0),), 0, device=device, dtype=torch.long)  # All fake_A are benign (label 0)\n",
        "        loss_class_fake_B = criterion_classification(class_fake_B, target_fake_B)\n",
        "        loss_class_fake_A = criterion_classification(class_fake_A, target_fake_A)\n",
        "\n",
        "        # Total loss for Generators\n",
        "        loss_G = (loss_id_A + loss_id_B + loss_GAN_A2B + loss_GAN_B2A +\n",
        "                  loss_cycle_ABA + loss_cycle_BAB + loss_class_fake_B + loss_class_fake_A)\n",
        "        loss_G.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # -----------------------\n",
        "        #  Train Discriminator D_A\n",
        "        # -----------------------\n",
        "        optimizer_D_A.zero_grad()\n",
        "\n",
        "        # Real loss\n",
        "        pred_real_A, class_real_A = netD_A(real_A)\n",
        "        loss_D_real_A = criterion_GAN(pred_real_A, torch.ones(pred_real_A.size(), device=device))\n",
        "        target_real_A = torch.full((class_real_A.size(0),), 0, device=device, dtype=torch.long)  # All real_A are benign (label 0)\n",
        "        loss_class_real_A = criterion_classification(class_real_A, target_real_A)\n",
        "\n",
        "        # Fake loss (detach to avoid training G on these labels)\n",
        "        pred_fake_A, class_fake_A = netD_A(fake_A.detach())\n",
        "        loss_D_fake_A = criterion_GAN(pred_fake_A, torch.zeros(pred_fake_A.size(), device=device))\n",
        "        loss_class_fake_A = criterion_classification(class_fake_A, target_fake_A)\n",
        "\n",
        "        # Total loss for Discriminator A\n",
        "        loss_D_A = (loss_D_real_A + loss_D_fake_A + loss_class_real_A + loss_class_fake_A) / 2\n",
        "        loss_D_A.backward()\n",
        "        optimizer_D_A.step()\n",
        "\n",
        "        # -----------------------\n",
        "        #  Train Discriminator D_B\n",
        "        # -----------------------\n",
        "        optimizer_D_B.zero_grad()\n",
        "\n",
        "        # Real loss\n",
        "        pred_real_B, class_real_B = netD_B(real_B)\n",
        "        loss_D_real_B = criterion_GAN(pred_real_B, torch.ones(pred_real_B.size(), device=device))\n",
        "        target_real_B = torch.full((class_real_B.size(0),), 1, device=device, dtype=torch.long)  # All real_B are malignant (label 1)\n",
        "        loss_class_real_B = criterion_classification(class_real_B, target_real_B)\n",
        "\n",
        "        # Fake loss (detach to avoid training G on these labels)\n",
        "        pred_fake_B, class_fake_B = netD_B(fake_B.detach())\n",
        "        loss_D_fake_B = criterion_GAN(pred_fake_B, torch.zeros(pred_fake_B.size(), device=device))\n",
        "        loss_class_fake_B = criterion_classification(class_fake_B, target_fake_B)\n",
        "\n",
        "        # Total loss for Discriminator B\n",
        "        loss_D_B = (loss_D_real_B + loss_D_fake_B + loss_class_real_B + loss_class_fake_B) / 2\n",
        "        loss_D_B.backward()\n",
        "        optimizer_D_B.step()\n",
        "\n",
        "        # ---------------------\n",
        "        #  Log Progress\n",
        "        # ---------------------\n",
        "        print(f\"Epoch [{epoch}/{num_epochs}] Batch {i}/{len(loader_A)} \\\n",
        "              Loss D_A: {loss_D_A.item()}, Loss D_B: {loss_D_B.item()} \\\n",
        "              Loss G: {loss_G.item()}\")\n",
        "\n",
        "        # If at save interval => save generated image samples\n",
        "        if i % 20 == 0:  # For example, visualize every 20 batches\n",
        "            plot_single_real_and_fake_image(real_A[0], fake_B[0])  # Pass the first image of the batch\n",
        "            plot_single_real_and_fake_image(real_B[0], fake_A[0])\n",
        "\n",
        "            # Predict class of images and print\n",
        "            _, class_real_A = netD_A(real_A)\n",
        "            _, class_real_B = netD_B(real_B)\n",
        "            _, class_fake_A = netD_A(fake_A)\n",
        "            _, class_fake_B = netD_B(fake_B)\n",
        "\n",
        "            pred_class_real_A = class_real_A.argmax(dim=1).item()\n",
        "            pred_class_real_B = class_real_B.argmax(dim=1).item()\n",
        "            pred_class_fake_A = class_fake_A.argmax(dim=1).item()\n",
        "            pred_class_fake_B = class_fake_B.argmax(dim=1).item()\n",
        "\n",
        "            print(f\"Predicted class for real_A: {'Benign' if pred_class_real_A == 0 else 'Malignant'}\")\n",
        "            print(f\"Predicted class for real_B: {'Benign' if pred_class_real_B == 0 else 'Malignant'}\")\n",
        "            print(f\"Predicted class for fake_A: {'Benign' if pred_class_fake_A == 0 else 'Malignant'}\")\n",
        "            print(f\"Predicted class for fake_B: {'Benign' if pred_class_fake_B == 0 else 'Malignant'}\")\n",
        "\n",
        "    # Record the end time of the epoch\n",
        "    epoch_end_time = time.time()\n",
        "    epoch_duration = epoch_end_time - epoch_start_time\n",
        "    print(f\"Epoch {epoch} completed in {epoch_duration:.2f} seconds\")\n",
        "\n",
        "    # Update learning rates\n",
        "    # lr_scheduler_G.step()\n",
        "    # lr_scheduler_D_A.step()\n",
        "    # lr_scheduler_D_B.step()\n",
        "\n",
        "    # Save models at the end of each epoch or at specific intervals\n",
        "    if (epoch + 1) % 1 == 0:  # Every epoch\n",
        "       save_models_to_drive(epoch, netG_A2B, netG_B2A, netD_A, netD_B)\n",
        "\n",
        "# After the final epoch, save the refined generated images\n",
        "# save_final_generated_images(G, dataloader, classifier, epoch=num_epochs, base_directory=\"output_breakhis/final_images\", device=device)\n",
        "\n",
        "# Record the total training end time\n",
        "total_training_end_time = time.time()\n",
        "total_training_duration = total_training_end_time - total_training_start_time\n",
        "print(f\"Total training time: {total_training_duration:.2f} seconds\")"
      ],
      "metadata": {
        "id": "THo10hNjVlSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mEHDaVFrd_mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NoOrVhKlcEgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tNs9SkLtcEUV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}