{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## üîß Library Imports\n",
        "\n",
        "We begin by importing essential libraries required for training a CycleGAN using PyTorch. These include modules for neural network construction (`torch.nn`), optimization (`torch.optim`), image preprocessing (`torchvision.transforms`), dataset loading, and image manipulation via PIL. We also import helper modules for dynamic computation graphs and data iteration.\n"
      ],
      "metadata": {
        "id": "S6s-FJtk-PkW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8fX4s2nkjlT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "import os\n",
        "import itertools\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üßπ Clear CUDA Cache\n",
        "\n",
        "Before training begins, we clear the CUDA memory cache to prevent potential memory issues on the GPU.\n"
      ],
      "metadata": {
        "id": "GX8-YuRc-Sro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear CUDA cache\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "yOdjSaES0ldt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üíª Select Compute Device\n",
        "\n",
        "We define the computation device, defaulting to GPU (`cuda`) if available, otherwise falling back to CPU. This enables seamless training on Colab‚Äôs hardware accelerators.\n"
      ],
      "metadata": {
        "id": "oW7p-Iu3-Tk-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XV443kLTk94H"
      },
      "outputs": [],
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìÅ Mount Google Drive\n",
        "\n",
        "To access the dataset stored in Google Drive, we mount the drive into the Colab runtime. This allows for persistent access to image folders and model checkpoints.\n"
      ],
      "metadata": {
        "id": "7LiBKp65-Wef"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmWix7oUlNCh"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üñºÔ∏è Load and Preprocess Image Data\n",
        "\n",
        "We define a preprocessing pipeline using `torchvision.transforms` to resize and normalize the images. We then load the benign and malignant training images using `ImageFolder`, and wrap them in `DataLoader` objects for efficient batch iteration during training.\n"
      ],
      "metadata": {
        "id": "BplLhJcz-boY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bA0E5-XslQpJ"
      },
      "outputs": [],
      "source": [
        "#Load Data\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(256),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "dataset_A = datasets.ImageFolder('/content/drive/MyDrive/my_data/train/benign', transform=transform)\n",
        "dataset_B = datasets.ImageFolder('/content/drive/MyDrive/my_data/train/malignant', transform=transform)\n",
        "\n",
        "loader_A = DataLoader(dataset_A, batch_size=1, shuffle=True)\n",
        "loader_B = DataLoader(dataset_B, batch_size=1, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß™ Visualize Sample Images\n",
        "\n",
        "To verify the correctness of the loaded data, we visualize one sample image each from the benign and malignant datasets. This step helps confirm that data loading and transformations are working as intended.\n"
      ],
      "metadata": {
        "id": "3zlGXrQs-dw4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WS8J-ldelW1s"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def imshow(img):\n",
        "    img = img.numpy().transpose((1, 2, 0))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "\n",
        "dataiter_A = iter(loader_A)\n",
        "images_A, _ = next(dataiter_A)\n",
        "\n",
        "dataiter_B = iter(loader_B)\n",
        "images_B, _ = next(dataiter_B)\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Sample from trainA (benign)')\n",
        "imshow(images_A[0])\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Sample from trainB (malignant)')\n",
        "imshow(images_B[0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚öôÔ∏è Hyperparameters\n",
        "\n",
        "We define key hyperparameters for model training, including input/output channels, image size, learning rate, and the number of residual blocks in the generator architecture.\n"
      ],
      "metadata": {
        "id": "SxYXlEVT-qkr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddqOS_NVk92o"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "input_nc = 3\n",
        "output_nc = 3\n",
        "size = 256\n",
        "#batch_size = 50\n",
        "lr = 0.0002\n",
        "beta1 = 0.5\n",
        "n_residual_blocks = 9"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† Define Generator Architecture with Residual Blocks\n",
        "\n",
        "Here we define the CycleGAN Generator model using a residual architecture. The generator includes:\n",
        "\n",
        "- An initial convolution block,\n",
        "- Downsampling layers,\n",
        "- Multiple residual blocks,\n",
        "- Upsampling layers,\n",
        "- An output layer with Tanh activation.\n",
        "\n",
        "Residual connections help retain feature integrity while transforming images across domains.\n"
      ],
      "metadata": {
        "id": "MUwftZsK-rv0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60zkRNRik9tC"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_features):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(in_features, in_features, 3),\n",
        "            nn.InstanceNorm2d(in_features),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(in_features, in_features, 3),\n",
        "            nn.InstanceNorm2d(in_features)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.block(x)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_nc, output_nc, n_residual_blocks=9):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        # Initial convolution block\n",
        "        model = [nn.ReflectionPad2d(3),\n",
        "                 nn.Conv2d(input_nc, 64, 7),\n",
        "                 nn.InstanceNorm2d(64),\n",
        "                 nn.ReLU(inplace=True)]\n",
        "\n",
        "        # Downsampling\n",
        "        in_features = 64\n",
        "        out_features = in_features*2\n",
        "        for _ in range(2):\n",
        "            model += [nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
        "                      nn.InstanceNorm2d(out_features),\n",
        "                      nn.ReLU(inplace=True)]\n",
        "            in_features = out_features\n",
        "            out_features = in_features*2\n",
        "\n",
        "        # Residual blocks\n",
        "        for _ in range(n_residual_blocks):\n",
        "            model += [ResidualBlock(in_features)]\n",
        "\n",
        "        # Upsampling\n",
        "        out_features = in_features//2\n",
        "        for _ in range(2):\n",
        "            model += [nn.ConvTranspose2d(in_features, out_features, 3, stride=2, padding=1, output_padding=1),\n",
        "                      nn.InstanceNorm2d(out_features),\n",
        "                      nn.ReLU(inplace=True)]\n",
        "            in_features = out_features\n",
        "            out_features = in_features//2\n",
        "\n",
        "        # Output layer\n",
        "        model += [nn.ReflectionPad2d(3),\n",
        "                  nn.Conv2d(64, output_nc, 7),\n",
        "                  nn.Tanh()]\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üïµÔ∏è Define Discriminator Architecture\n",
        "\n",
        "We implement the PatchGAN discriminator, which evaluates the realism of image patches instead of full images. It consists of stacked convolutional layers with increasing depth, enabling the model to distinguish between real and translated images.\n"
      ],
      "metadata": {
        "id": "5ivHFGQu-vHI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JH16gLx6k9rq"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_nc):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        model = [nn.Conv2d(input_nc, 64, 4, stride=2, padding=1),\n",
        "                 nn.LeakyReLU(0.2, inplace=True)]\n",
        "\n",
        "        model += [nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
        "                  nn.InstanceNorm2d(128),\n",
        "                  nn.LeakyReLU(0.2, inplace=True)]\n",
        "\n",
        "        model += [nn.Conv2d(128, 256, 4, stride=2, padding=1),\n",
        "                  nn.InstanceNorm2d(256),\n",
        "                  nn.LeakyReLU(0.2, inplace=True)]\n",
        "\n",
        "        model += [nn.Conv2d(256, 512, 4, padding=1),\n",
        "                  nn.InstanceNorm2d(512),\n",
        "                  nn.LeakyReLU(0.2, inplace=True)]\n",
        "\n",
        "        # Output layer\n",
        "        model += [nn.Conv2d(512, 1, 4, padding=1)]\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üõ†Ô∏è Initialize Generator and Discriminator Models\n",
        "\n",
        "We instantiate two generators (A‚ÜíB and B‚ÜíA) and two discriminators (one for each domain), moving all models to the selected compute device (CPU or GPU).\n"
      ],
      "metadata": {
        "id": "UozGFAnP-xRp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyRakJK9k9yj"
      },
      "outputs": [],
      "source": [
        "# Initialize models\n",
        "netG_A2B = Generator(input_nc, output_nc, n_residual_blocks).to(device)\n",
        "netG_B2A = Generator(input_nc, output_nc, n_residual_blocks).to(device)\n",
        "netD_A = Discriminator(input_nc).to(device)\n",
        "netD_B = Discriminator(input_nc).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ Define Optimizers and Loss Functions\n",
        "\n",
        "We set up separate Adam optimizers for the generators and discriminators. The losses used are:\n",
        "\n",
        "- **MSE Loss** for adversarial learning,\n",
        "- **L1 Loss** for cycle-consistency and identity constraints,\n",
        "- **CrossEntropy Loss** (prepared for optional classification tasks).\n"
      ],
      "metadata": {
        "id": "XdHtiiVA-5gB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6F8wDozk9xF"
      },
      "outputs": [],
      "source": [
        "# Optimizers\n",
        "optimizer_G = optim.Adam(itertools.chain(netG_A2B.parameters(), netG_B2A.parameters()), lr=lr, betas=(beta1, 0.999))\n",
        "optimizer_D_A = optim.Adam(netD_A.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optimizer_D_B = optim.Adam(netD_B.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "\n",
        "# Define loss functions\n",
        "criterion_GAN = nn.MSELoss().to(device)\n",
        "criterion_cycle = nn.L1Loss().to(device)\n",
        "criterion_identity = nn.L1Loss().to(device)\n",
        "criterion_classification = nn.CrossEntropyLoss().to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üñºÔ∏è Utility: Visualize Translated Images\n",
        "\n",
        "This helper function allows us to visualize real and fake images during training. It shows side-by-side comparisons between real images from both domains and their translated counterparts.\n"
      ],
      "metadata": {
        "id": "_1SguNVF-7wu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OCLhT42k9lz"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "\n",
        "def visualize_generated_images(real_A, real_B, fake_A, fake_B):\n",
        "    \"\"\"Visualize one sample translation from both generators.\"\"\"\n",
        "    with torch.no_grad():\n",
        "        # Assuming normalization was used\n",
        "        real_A = 0.5 * (real_A + 1.0)\n",
        "        real_B = 0.5 * (real_B + 1.0)\n",
        "        fake_A = 0.5 * (fake_A + 1.0)\n",
        "        fake_B = 0.5 * (fake_B + 1.0)\n",
        "\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        plt.subplot(221)\n",
        "        plt.title(\"Real A\")\n",
        "        plt.imshow(real_A[0].cpu().permute(1, 2, 0))\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(222)\n",
        "        plt.title(\"Fake B\")\n",
        "        plt.imshow(fake_B[0].cpu().permute(1, 2, 0))\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(223)\n",
        "        plt.title(\"Real B\")\n",
        "        plt.imshow(real_B[0].cpu().permute(1, 2, 0))\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(224)\n",
        "        plt.title(\"Fake A\")\n",
        "        plt.imshow(fake_A[0].cpu().permute(1, 2, 0))\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üíæ Save Model Checkpoints to Google Drive\n",
        "\n",
        "To preserve training progress, this function saves the state dictionaries of all generator and discriminator models after each epoch to a specified directory in Google Drive.\n"
      ],
      "metadata": {
        "id": "w8aELjgI-_lz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5Dq8K_HlzjI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "def save_models_to_drive(epoch, netG_A2B, netG_B2A, netD_A, netD_B, drive_path='/content/drive/MyDrive/CycleGAN_Models'):\n",
        "    \"\"\"\n",
        "    Save model parameters to Google Drive.\n",
        "\n",
        "    Parameters:\n",
        "        epoch (int): The current epoch number.\n",
        "        netG_A2B (nn.Module): Generator model from domain A to B.\n",
        "        netG_B2A (nn.Module): Generator model from domain B to A.\n",
        "        netD_A (nn.Module): Discriminator model for domain A.\n",
        "        netD_B (nn.Module): Discriminator model for domain B.\n",
        "        drive_path (str): The path in Google Drive to save the models.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(drive_path):\n",
        "        os.makedirs(drive_path)\n",
        "\n",
        "    # Define file paths for saving\n",
        "    path_G_A2B = os.path.join(drive_path, f'netG_A2B_epoch_{epoch}.pth')\n",
        "    path_G_B2A = os.path.join(drive_path, f'netG_B2A_epoch_{epoch}.pth')\n",
        "    path_D_A = os.path.join(drive_path, f'netD_A_epoch_{epoch}.pth')\n",
        "    path_D_B = os.path.join(drive_path, f'netD_B_epoch_{epoch}.pth')\n",
        "\n",
        "    # Save the models\n",
        "    torch.save(netG_A2B.state_dict(), path_G_A2B)\n",
        "    torch.save(netG_B2A.state_dict(), path_G_B2A)\n",
        "    torch.save(netD_A.state_dict(), path_D_A)\n",
        "    torch.save(netD_B.state_dict(), path_D_B)\n",
        "\n",
        "    print(f\"Saved models at epoch {epoch} to {drive_path}\")\n",
        "\n",
        "# Example usage within the training loop:\n",
        "# save_models_to_drive(epoch, netG_A2B, netG_B2A, netD_A, netD_B)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear CUDA cache\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "owbekq0r06Lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéØ Training Loop: CycleGAN for Domain Translation\n",
        "\n",
        "We now train the CycleGAN model for a specified number of epochs. The training process includes:\n",
        "\n",
        "- **Identity Loss**: Preserves features when input and output domains are the same.\n",
        "- **Adversarial Loss**: Guides the generators to produce realistic images.\n",
        "- **Cycle-consistency Loss**: Ensures image transformation is reversible.\n",
        "- **Discriminator Updates**: Enables learning to distinguish real from generated images.\n",
        "\n",
        "We also include real-time visualization and periodic saving of model checkpoints to Google Drive. Training durations are tracked per epoch and in total.\n"
      ],
      "metadata": {
        "id": "VjkTHuxc_CmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import itertools\n",
        "from torch.autograd import Variable\n",
        "import time\n",
        "\n",
        "# Parameters\n",
        "num_epochs = 25\n",
        "lambda_id = 0.1\n",
        "lambda_cycle = 10\n",
        "\n",
        "# Record the total training start time\n",
        "total_training_start_time = time.time()\n",
        "\n",
        "\n",
        "# Path to save images in Google Drive\n",
        "#drive_image_path = '/content/drive/My Drive/CycleGAN_Images'\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    # Record the start time of the epoch\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    for i, (data_A, data_B) in enumerate(zip(loader_A, loader_B)):\n",
        "        real_A = Variable(data_A[0].to(device))\n",
        "        real_B = Variable(data_B[0].to(device))\n",
        "\n",
        "        # ----------------------\n",
        "        #  Train Generators A2B and B2A\n",
        "        # ----------------------\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # Identity loss\n",
        "        same_B = netG_A2B(real_B)\n",
        "        loss_identity_B = criterion_identity(same_B, real_B) * lambda_cycle * lambda_id\n",
        "        same_A = netG_B2A(real_A)\n",
        "        loss_identity_A = criterion_identity(same_A, real_A) * lambda_cycle * lambda_id\n",
        "\n",
        "        # GAN loss\n",
        "        fake_B = netG_A2B(real_A)\n",
        "        pred_fake = netD_B(fake_B)\n",
        "        loss_GAN_A2B = criterion_GAN(pred_fake, Variable(torch.ones(pred_fake.size()).to(device)))\n",
        "\n",
        "        fake_A = netG_B2A(real_B)\n",
        "        pred_fake = netD_A(fake_A)\n",
        "        loss_GAN_B2A = criterion_GAN(pred_fake, Variable(torch.ones(pred_fake.size()).to(device)))\n",
        "\n",
        "        # Cycle loss\n",
        "        recovered_A = netG_B2A(fake_B)\n",
        "        loss_cycle_ABA = criterion_cycle(recovered_A, real_A) * lambda_cycle\n",
        "\n",
        "        recovered_B = netG_A2B(fake_A)\n",
        "        loss_cycle_BAB = criterion_cycle(recovered_B, real_B) * lambda_cycle\n",
        "\n",
        "        # Total loss\n",
        "        total_loss_G = loss_identity_A + loss_identity_B + loss_GAN_A2B + loss_GAN_B2A + loss_cycle_ABA + loss_cycle_BAB\n",
        "        total_loss_G.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # -----------------------\n",
        "        #  Train Discriminator D_A\n",
        "        # -----------------------\n",
        "        optimizer_D_A.zero_grad()\n",
        "\n",
        "        # Real loss\n",
        "        pred_real = netD_A(real_A)\n",
        "        loss_D_real = criterion_GAN(pred_real, Variable(torch.ones(pred_real.size()).to(device)))\n",
        "\n",
        "        # Fake loss\n",
        "        pred_fake = netD_A(fake_A.detach())\n",
        "        loss_D_fake = criterion_GAN(pred_fake, Variable(torch.zeros(pred_fake.size()).to(device)))\n",
        "\n",
        "        # Total loss\n",
        "        loss_D_A = (loss_D_real + loss_D_fake) * 0.5\n",
        "        loss_D_A.backward()\n",
        "        optimizer_D_A.step()\n",
        "\n",
        "        # -----------------------\n",
        "        #  Train Discriminator D_B\n",
        "        # -----------------------\n",
        "        optimizer_D_B.zero_grad()\n",
        "\n",
        "        # Real loss\n",
        "        pred_real = netD_B(real_B)\n",
        "        loss_D_real = criterion_GAN(pred_real, Variable(torch.ones(pred_real.size()).to(device)))\n",
        "\n",
        "        # Fake loss\n",
        "        pred_fake = netD_B(fake_B.detach())\n",
        "        loss_D_fake = criterion_GAN(pred_fake, Variable(torch.zeros(pred_fake.size()).to(device)))\n",
        "\n",
        "        # Total loss\n",
        "        loss_D_B = (loss_D_real + loss_D_fake) * 0.5\n",
        "        loss_D_B.backward()\n",
        "        optimizer_D_B.step()\n",
        "\n",
        "        # Visualize generated images periodically\n",
        "        if i % 100 == 0:  # Adjust this value based on your training dataset size\n",
        "            visualize_generated_images(real_A, real_B, fake_A, fake_B)\n",
        "\n",
        "        # Print training progress\n",
        "        if i % 10 == 0:  # Adjust the printing frequency as needed\n",
        "            print(f\"Epoch {epoch}/{num_epochs} - Batch {i}/{min(len(loader_A), len(loader_B))}, \"\n",
        "                  f\"Loss_G: {total_loss_G.item()}, Loss_D_A: {loss_D_A.item()}, Loss_D_B: {loss_D_B.item()}\")\n",
        "\n",
        "    # Record the end time of the epoch\n",
        "    epoch_end_time = time.time()\n",
        "    epoch_duration = epoch_end_time - epoch_start_time\n",
        "    print(f\"Epoch {epoch} completed in {epoch_duration:.2f} seconds\")\n",
        "\n",
        "# Save models at the end of each epoch or at specific intervals\n",
        "if (epoch + 1) % 1 == 0:  # Every 10 epochs\n",
        "    save_models_to_drive(epoch, netG_A2B, netG_B2A, netD_A, netD_B)\n",
        "\n",
        "\n",
        "# Record the total training end time\n",
        "total_training_end_time = time.time()\n",
        "total_training_duration = total_training_end_time - total_training_start_time\n",
        "print(f\"Total training time: {total_training_duration:.2f} seconds\")\n",
        "\n",
        "    # Optionally, you can also visualize at the end of each epoch to see the progress\n",
        "    #visualize_generated_images(real_A, real_B, fake_A, fake_B)\n",
        "\n",
        "    # Evaluate the model after each epoch\n",
        "    #evaluation_results = evaluate_model(netG_A2B, netG_B2A, loader_A, loader_B, device)\n",
        "\n",
        "    # Print evaluation results\n",
        "    #print(f\"Epoch {epoch + 1} Evaluation Results: {evaluation_results}\")\n",
        "\n",
        "\n",
        "# Save generated images to Google Drive every 30 epochs\n",
        "#if (epoch + 1) % 30 == 0:\n",
        "      #epoch_dir = os.path.join(drive_image_path, f'epoch_{epoch + 1}')\n",
        "      #save_image_to_drive(fake_A, epoch_dir, f'fake_A_epoch_{epoch + 1}.png')\n",
        "      #save_image_to_drive(fake_B, epoch_dir, f'fake_B_epoch_{epoch + 1}.png')\n",
        "      #save_image_to_drive(real_A, epoch_dir, f'real_A_epoch_{epoch + 1}.png')\n",
        "      #save_image_to_drive(real_B, epoch_dir, f'real_B_epoch_{epoch + 1}.png')\n",
        "      #save_image_to_drive(recovered_A, epoch_dir, f'recovered_A_epoch_{epoch + 1}.png')\n",
        "      #save_image_to_drive(recovered_B, epoch_dir, f'recovered_B_epoch_{epoch + 1}.png')\n"
      ],
      "metadata": {
        "id": "NI5LVnTmQdHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NWcG9P3vQc-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HhnUFXJwtwT6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}