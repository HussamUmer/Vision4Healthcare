# ğŸ“¡ UltraSeg-Bench: AbdomenUSMS Comparative Study

![License](https://img.shields.io/badge/License-MIT-green.svg)
![Python](https://img.shields.io/badge/Python-%E2%89%A53.9-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.x-red)
![Made with â¤ï¸](https://img.shields.io/badge/Made%20with-%E2%9D%A4-ff69b4)

>  A small, carefully controlled **comparative study** of three segmentation baselines â€” **U-Net**, **Attention U-Net**, and **DeepLabV3+** â€” on the **AbdomenUSMSBench** (ultrasound) subset of **MedSegBench**. We focus on **reproducibility**, **fairness**, and **clear reporting** (val-selected best checkpoints, identical splits/metrics).

---

## âœ¨ Why this study?

Ultrasound segmentation is **hard**: speckle noise, low contrast, and small/ambiguous organs make generalization tricky. We wanted a **practical, Colab-friendly baseline** that:
- compares **classic U-Net**, **attention-gated U-Net**, and **DeepLabV3+** under the **same pipeline**,
- uses **val mDice** to select best checkpoints and reports **test** performance fairly,
- is **reproducible** (fixed seeds, deterministic settings where possible).

---

## ğŸ“¦ Dataset

**AbdomenUSMSBench** (MedSegBench) â€” abdominal **ultrasound** multi-structure segmentation.  
- Format: images with pixel-wise class labels (background + organs).  
- We use the **256Ã—256** preprocessed NPZ release for Colab friendliness.  
- **Citation / Source:** MedSegBench repository + Zenodo data release (AbdomenUS 256).  
  - Zenodo data file (as used here): `abdomenus_256.npz`  
  - Zenodo record link (download): https://zenodo.org/records/13358372/files/abdomenus_256.npz?download=1  
  - Project page: https://github.com/zekikus/MedSegBench

> âš ï¸ Please respect the datasetâ€™s license and terms. This repo **does not** redistribute data; the notebooks **download directly** from the official Zenodo link at runtime.

**Why itâ€™s challenging:** ultrasound artifacts (speckle), low SNR, fuzzy boundaries, and **class imbalance** (background dominance, small/rare organs). These factors penalize models that lack strong context modeling or robust regularization.
Here is a single sample from each set:

>Train Sample:

![Predict vs Truth â€” Example#1](https://github.com/HussamUmer/Vision4Healthcare/blob/main/UltraSeg-Bench%3A%20AbdomenUSMS%20Comparative%20Study/Output/Dataset/download%20(2).png?raw=1)

>Validation Sample:
![Predict vs Truth â€” Example#2](https://github.com/HussamUmer/Vision4Healthcare/blob/main/UltraSeg-Bench%3A%20AbdomenUSMS%20Comparative%20Study/Output/Dataset/download%20(3).png?raw=1)


>Test Sample:
![Predict vs Truth â€” Example#2](https://github.com/HussamUmer/Vision4Healthcare/blob/main/UltraSeg-Bench%3A%20AbdomenUSMS%20Comparative%20Study/Output/Dataset/download%20(4).png?raw=1)

<p align="center">
<br><em>Figure: Representative samples from the AbdomenUS dataset across splitsâ€”Train, Validation, and Test. Each panel shows the ultrasound image with ground-truth overlay , using a consistent class color palette (background hidden) to illustrate appearance, annotation quality, and split similarity.</em>
</p>

---

## ğŸ§ª Methods (all Colab-ready)

- **U-Net** (scratch)  
- **Attention U-Net** (scratch; attention gating on skip connections)  
- **DeepLabV3+** (ImageNet-pretrained ResNet-50 backbone; ASPP multi-scale context)

All three share the **same steps**:
1. Setup (installs, imports), **seeding** (Python/NumPy/Torch), deterministic cuDNN.
2. Manual dataset fetch + **MD5** check to avoid flaky auto-download.
3. Load **train/val/test** splits; count samples.
4. Visualize images, masks, overlays (legendless for simplicity).
5. **NumPy-safe** paired augmentations (flips; optional light affine/intensity).
6. Torch `Dataset`/`DataLoader` (conditional `pin_memory`, `drop_last` for BN-safety).
7. **Losses:** class-weighted CrossEntropy (inverse frequency) + **soft Dice**.  
   **Metrics:** mIoU, mDice (ignore background in metrics).
8. **Training:** AMP (guarded), AdamW, ReduceLROnPlateau, early stopping, **best-on-val-Dice** checkpointing.
9. **Evaluation:** test metrics (optionally with horizontal-flip **TTA**).  
10. **Visualization:** qualitative predictions (overlay).  
11. **Reproducibility:** history saved (`.npz`, `.json`) + plotting cells.

---

## ğŸš€ Quick start (Colab)

Click a badge to open notebooks in colab:

- U-Net:
[![Open In Colab â€” AbdomenUSMSBench Attention U-Net](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/HussamUmer/Vision4Healthcare/blob/main/UltraSeg-Bench%3A%20AbdomenUSMS%20Comparative%20Study/Notebooks/AbdomenUSMSBench_UNet.ipynb)
- Attention U-Net: 
[![Open In Colab â€” AbdomenUSMSBench U-Net](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/HussamUmer/Vision4Healthcare/blob/main/UltraSeg-Bench%3A%20AbdomenUSMS%20Comparative%20Study/Notebooks/AbdomenUSMSBench_AttentionUNet.ipynb)
- DeepLabV3+:
[![Open In Colab â€” AbdomenUSMSBench DeepLabV3](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/HussamUmer/Vision4Healthcare/blob/main/UltraSeg-Bench%3A%20AbdomenUSMS%20Comparative%20Study/Notebooks/AbdomenUSMSBench_DeepLabV3.ipynb)



---

## ğŸ“Š Results

### 1) Best-Checkpoint Summary (Validation-selected â†’ Tested on held-out test set)

| Model | Val mDice | Val mIoU | Val Loss | Test mDice | Test mIoU | Test Loss |
|---|---:|---:|---:|---:|---:| ---:|
| U-Net | 0.6059 | 0.5075 | 0.6035 |0.4868 | 0.4056 | 0.7915 |
| DeepLabV3+ | 0.6934 | 0.6186 | 0.5665 | 0.5565 | 0.4821 | 0.8345|
| Attention U-Net | 0.6227 | 0.5398 | 0.7008 | 0.5674 | 0.4906 | 0.8844|

**Reading the table:** We select the **best checkpoint by validation mDice** for each model, then compute **test** metrics with the same evaluation settings. **Attention U-Net** generalizes best on this dataset (highest **test mDice/mIoU**), while **DeepLabV3+** shows stronger **val** performance but a larger generalization gap. **U-Net** (baseline) trails both.

<p align="center">
  <img src="https://github.com/HussamUmer/Vision4Healthcare/blob/main/UltraSeg-Bench%3A%20AbdomenUSMS%20Comparative%20Study/Output/OverallComparisonGraph/newplot.png?raw=1" alt="Best-Checkpoint Summary â€” Val vs Test metrics across models" width="880">
  <br><em>Figure: Validation-selected best checkpoints vs. held-out test performance (mDice, mIoU, and loss) for U-Net, Attention U-Net, and DeepLabV3+.</em>
</p>


---

### 2) Generalization Gaps (smaller is better)

| Model | Trainâ†’Test Î”Dice | Valâ†’Test Î”Dice | Trainâ†’Test Î”IoU | Valâ†’Test Î”IoU |
|---|---:|---:|---:|---:|
| U-Net | 0.6927 â†’ 0.4868 = **0.2059** | 0.6059 â†’ 0.4868 = **0.1191** | 0.6087 â†’ 0.4056 = **0.2031** | 0.5075 â†’ 0.4056 = **0.1019** |
| **Attention U-Net** | 0.6705 â†’ 0.5674 = **0.1031** | 0.6227 â†’ 0.5674 = **0.0553** | 0.5868 â†’ 0.4906 = **0.0962** | 0.5398 â†’ 0.4906 = **0.0492** |
| DeepLabV3+ | 0.8531 â†’ 0.5565 = **0.2966** | 0.6934 â†’ 0.5565 = **0.1369** | 0.8102 â†’ 0.4821 = **0.3281** | 0.6186 â†’ 0.4821 = **0.1365** |

**Interpretation**: **Attention U-Net** has the smallest gaps across Dice and IoU, indicating the strongest generalization among the three under my current setup.

<p align="center">
  <img src="https://github.com/HussamUmer/Vision4Healthcare/blob/main/UltraSeg-Bench%3A%20AbdomenUSMS%20Comparative%20Study/Output/OverallComparisonGraph/newplot%20(1).png?raw=1" alt="Generalization Gaps across models (mDice & mIoU)" width="880">
  <br><em>Figure: Generalization gaps (Trainâ†’Test and Valâ†’Test) for mDice and mIoU â€” smaller is better; Attention U-Net shows the tightest gaps.</em>
</p>

---

### 3) Efficiency

| Model | Total Epochs Run | Best Epoch |
|---|---:|---:|
| U-Net | 100 | 99 | 
| **Attention U-Net** | 57 | 49 |
| DeepLabV3+ | 46 | 26 |

**Interpretation:** With early stopping/scheduling, **Attention U-Net** converges efficiently; **DeepLabV3+** reaches best quickly but generalizes slightly worse than AttU-Net on test.

<p align="center">
  <img src="https://github.com/HussamUmer/Vision4Healthcare/blob/main/UltraSeg-Bench%3A%20AbdomenUSMS%20Comparative%20Study/Output/OverallComparisonGraph/newplot%20(2).png?raw=1" alt="Training efficiency â€” total epochs and best-epoch per model" width="880">
  <br><em>Figure: Training efficiency across models â€” total epochs run and the epoch at which the best validation mDice was achieved (lower is better).</em>
</p>


---

## ğŸ–¼ï¸ Visuals (qualitative)

Below are representative cases from the test split. For each case we show the same image across models so we can compare boundaries, small structures, and failure modes at a glance. Overlays use a consistent class color palette; background is hidden.

*These visuals are illustrative and complement the quantitative metrics; they highlight where models agree, where boundaries are uncertain, and which organs are most challenging.*

![Predict vs Truth â€” Example#1](https://github.com/HussamUmer/Vision4Healthcare/blob/main/UltraSeg-Bench%3A%20AbdomenUSMS%20Comparative%20Study/Output/PredictVsTruth/download%20(10).png?raw=1)

![Predict vs Truth â€” Example#2](https://github.com/HussamUmer/Vision4Healthcare/blob/main/UltraSeg-Bench%3A%20AbdomenUSMS%20Comparative%20Study/Output/PredictVsTruth/download%20(11).png?raw=1)

![Predict vs Truth â€” Example#3](https://github.com/HussamUmer/Vision4Healthcare/blob/main/UltraSeg-Bench%3A%20AbdomenUSMS%20Comparative%20Study/Output/PredictVsTruth/download%20(12).png?raw=1)

<p align="center">
  <br><em>Figure: Representative qualitative comparisons on the AbdomenUS test set. Each panel shows the same case across modelsâ€”Input, Ground Truth, U-Net, Attention U-Net, and DeepLabV3+â€”with a consistent class color palette (background hidden) to highlight boundary quality, small-structure recovery, and typical failure modes.</em>
</p>



---

## ğŸ“ˆ Epoch-wise Metric Comparisons (Train & Val)

### mDice per epoch (higher is better)
| U-Net | Attention U-Net | DeepLabV3+ |
|---|---|---|
| <img src="https://github.com/HussamUmer/Vision4Healthcare/blob/main/UltraSeg-Bench%3A%20AbdomenUSMS%20Comparative%20Study/Output/TrainValGraphs/UNet/download%20(9).png?raw=1" width="320"> | <img src="https://github.com/HussamUmer/Vision4Healthcare/blob/main/UltraSeg-Bench%3A%20AbdomenUSMS%20Comparative%20Study/Output/TrainValGraphs/AttUNet/download%20(13).png?raw=1" width="320"> | <img src="https://github.com/HussamUmer/Vision4Healthcare/blob/main/UltraSeg-Bench%3A%20AbdomenUSMS%20Comparative%20Study/Output/TrainValGraphs/DeepLabV3/download%20(6).png?raw=1" width="320"> |

*Shows segmentation quality across training; trainâ€“val gaps hint at under/overfitting.*

---

### mIoU per epoch (higher is better)
| U-Net | Attention U-Net | DeepLabV3+ |
|---|---|---|
| <img src="https://github.com/HussamUmer/Vision4Healthcare/blob/main/UltraSeg-Bench%3A%20AbdomenUSMS%20Comparative%20Study/Output/TrainValGraphs/UNet/download%20(8).png?raw=1" width="320"> | <img src="https://github.com/HussamUmer/Vision4Healthcare/blob/main/UltraSeg-Bench%3A%20AbdomenUSMS%20Comparative%20Study/Output/TrainValGraphs/AttUNet/download%20(3).png?raw=1" width="320"> | <img src="https://github.com/HussamUmer/Vision4Healthcare/blob/main/UltraSeg-Bench%3A%20AbdomenUSMS%20Comparative%20Study/Output/TrainValGraphs/DeepLabV3/download%20(5).png?raw=1" width="320"> |

*mIoU complements mDice; consistent trends reinforce conclusions about stability and generalization.*

---

### Loss per epoch (lower is better)
| U-Net | Attention U-Net | DeepLabV3+ |
|---|---|---|
| <img src="https://github.com/HussamUmer/Vision4Healthcare/blob/main/UltraSeg-Bench%3A%20AbdomenUSMS%20Comparative%20Study/Output/TrainValGraphs/UNet/download%20(7).png?raw=1" width="320"> | <img src="https://github.com/HussamUmer/Vision4Healthcare/blob/main/UltraSeg-Bench%3A%20AbdomenUSMS%20Comparative%20Study/Output/TrainValGraphs/AttUNet/download%20(2).png?raw=1" width="320"> | <img src="https://github.com/HussamUmer/Vision4Healthcare/blob/main/UltraSeg-Bench%3A%20AbdomenUSMS%20Comparative%20Study/Output/TrainValGraphs/DeepLabV3/download%20(4).png?raw=1" width="320"> |

*Loss curves reflect optimization behavior; widening trainâ€“val gaps indicate overfitting, smooth plateaus indicate convergence.*



---

## ğŸ” Reproducibility

- Fixed seeds for `random`, `numpy`, `torch` (+ CUDA)  
- cuDNN deterministic + benchmarking disabled  
- History saved (`.npz`, `.json`) for all runs  
- Colab notebooks include **step-by-step**, **commented** cells

---

## ğŸš§ Limitations & Next Steps

- Ultrasound segmentation remains challenging; small/rare organs and fuzzy boundaries depress Dice/IoU.

- DeepLabV3+ shows strong capacity but needs stronger regularization.

Next steps:

- Try Tversky/Focal-Tversky mixes; class-balanced sampling.

- 512px training or 256â†’512 fine-tuning (VRAM permitting).

- Pretrained encoders for U-Net/Attention U-Net for a more apples-to-apples backbone comparison.

- Per-class deep-dive and targeted augmentations (elastic, intensity, speckle).

---

## ğŸ“š Citations & Acknowledgments

Dataset: MedSegBench â€” AbdomenUSMSBench (Abdomen ultrasound multi-structure).
Zenodo (data file used): abdomenus_256.npz â€” https://zenodo.org/records/13358372/files/abdomenus_256.npz?download=1

### Project: https://github.com/zekikus/MedSegBench

### Architectures:

```

- Ronneberger et al., U-Net: Convolutional Networks for Biomedical Image Segmentation (MICCAI 2015).

- Oktay et al., Attention U-Net (ArXiv 2018).

- Chen et al., Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation (DeepLabV3+) (ECCV 2018).

If you use this repository, please cite the dataset and the original method papers above.

```

---

## ğŸ“„ License

This project is released under the MIT License. See LICENSE
 for details.



